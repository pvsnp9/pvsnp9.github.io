<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>RAG - Retrieval Augmented Generation | Tsuyog Basnet</title> <meta name="author" content="Tsuyog Basnet"> <meta name="description" content="A Guide to understand and build RAG application, with Embedding, Sentence BERT, Vector Database, and LLMs"> <meta name="keywords" content="Machine-Learninng, Data-Scientist, researcher, NLP, personal-blog"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pvsnp9.github.io/blog/2024/rag/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "RAG - Retrieval Augmented Generation",
      "description": "A Guide to understand and build RAG application, with Embedding, Sentence BERT, Vector Database, and LLMs",
      "published": "April 12, 2024",
      "authors": [
        {
          "author": "Tsuyog Basnet",
          "authorURL": "https://www.linkedin.com/in/tsuyog/",
          "affiliations": [
            {
              "name": "Vector Lab",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Tsuyog Basnet</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Experiments</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RAG - Retrieval Augmented Generation</h1> <p>A Guide to understand and build RAG application, with Embedding, Sentence BERT, Vector Database, and LLMs</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#large-language-models">Large Language Models</a></div> <ul> <li><a href="#fine-tuning">Fine Tuning</a></li> </ul> <div><a href="#prompt-engineering">Prompt Engineering</a></div> <div><a href="#rag-pipeline">RAG Pipeline</a></div> <div><a href="#embedding-vectors">Embedding Vectors</a></div> <ul> <li><a href="#sentence-embedding">Sentence Embedding</a></li> </ul> <div><a href="#sentence-bert">Sentence BERT</a></div> <div><a href="#vector-database">Vector Database</a></div> <div><a href="#code">Code</a></div> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#citations">Citations</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <blockquote> <p><strong>AI: I am sorry, i can not provide you the answer without context or I was trained on before date.</strong></p> </blockquote> <p>Retrieval Augmented Generation (RAG) combines retrieval-based and generative models in natural language processing to produce contextually relevant and coherent responses by first retrieving relevant passages or documents and then using them to guide the generative model.</p> <p>At its core, Retrieval Augmented Generation is a fusion of two fundamental approaches in NLP:</p> <ol> <li> <p><strong>Retrieval-Based Models</strong>: These models excel at retrieving relevant information from large corpora of text based on a given query or context. They leverage techniques like similarity search, or advanced methods such as dense retrieval with neural networks to efficiently fetch passages that are most likely to contain the desired information. In another words, finding most relevant documents for a input query based on specified algorithm (cosine similarity, or any distnace metrics).</p> </li> <li> <p><strong>Generative Models</strong>: On the other hand, generative models, particularly those based on transformers like GPT (Generative Pre-trained Transformer), have demonstrated remarkable proficiency in generating human-like text. They operate by predicting the next word or token in a sequence based on the preceding context, often trained on vast amounts of text data.</p> </li> </ol> <p>Retrieval Augmented Generation seeks to harness the strengths of both these approaches by integrating retrieval-based systems into the generative pipeline. The basic idea is to first retrieve a set of relevant passages or documents from a knowledge source (such as a large text corpus or a knowledge graph) and then use these retrieved contexts to guide the generative model in producing a more informed and contextually appropriate response.</p> <p>Here’s how the process typically unfolds:</p> <ol> <li> <p><strong>Retrieval</strong>: Given an input query or context, a retrieval-based model is employed to fetch a set of relevant documents or passages from a knowledge base. This retrieval step is crucial for providing the generative model with a rich source of contextual information.</p> </li> <li> <p><strong>Generation</strong>: The retrieved passages serve as the context for the generative model, which then generates a response based not only on the original input but also on the retrieved knowledge. By incorporating this additional context, the generative model can produce responses that are more coherent, informative, and contextually relevant.</p> </li> <li> <p><strong>Ranking</strong>: In some implementations, a ranking mechanism may be employed to select the most suitable response among multiple candidates generated by the generative model. This step ensures that the final output is of the highest quality and relevance.</p> </li> </ol> <p>The beauty of Retrieval Augmented Generation lies in its ability to combine the depth of knowledge retrieval with the creativity of generative models, resulting in responses that are not only fluent and coherent but also grounded in factual accuracy and contextual understanding. This approach has numerous applications across various domains, including question answering, conversational agents, content generation, and more.</p> <hr> <h2 id="large-language-models">Large Language Models</h2> <p>Large Language Models (LLMs) stand as marvels of mathematical ingenuity intertwined with cutting-edge technology. At their core, LLMs rely on intricate neural network designs, often built upon the transformative Transformer architecture. These models boast layers of attention mechanisms, allowing them to adeptly capture intricate linguistic nuances and dependencies within text. One key mathematical concept underpinning LLMs is the notion of attention mechanisms, particularly prevalent in architectures like the Transformer model. Attention mechanisms enable the model to weigh the importance of different words or tokens in a sequence, allowing it to focus on relevant information while filtering out noise. This process involves matrix operations and vector manipulations, where attention scores are computed through dot products and softmax functions, creating a weighted representation of the input.</p> <p>Additionally, LLMs leverage sophisticated optimization algorithms, such as stochastic gradient descent and its variants, to iteratively adjust model parameters during training, minimizing a loss function that quantifies the disparity between predicted and actual text. Through these mathematical intricacies, LLMs harness the power of data and computation to transcend the boundaries of language understanding and generation.</p> <p>For instance, <code class="language-plaintext highlighter-rouge">P ["Nepal" | "Kathmandu is a city in ____"]</code>. In this example, <code class="language-plaintext highlighter-rouge">pormpt : "Kathmandu is a city in"</code> and prediction of LLM model <code class="language-plaintext highlighter-rouge">token: "Nepal"</code>. The LLM model simply spits out the probability of each word in predefined vocabulary. It accquires the knowledge such as context, relationship via training.</p> <p><strong>List of some LLMs</strong>:</p> <ol> <li> <strong>GPT-3.5</strong> and <strong>GPT-4</strong> by <strong>OpenAI</strong>: These models power applications like <strong>ChatGPT</strong> and <strong>Microsoft Copilot</strong>².</li> <li> <strong>PaLM</strong> by <strong>Google</strong>: A commercial LLM.</li> <li> <strong>Gemini</strong> by Google: Currently used in the chatbot of the same name.</li> <li> <strong>Grok</strong> by <strong>xAI</strong>: An intriguing LLM.</li> <li> <strong>LLaMA</strong> family of open-source models by <strong>Meta</strong>.</li> <li> <strong>Claude</strong> models by <strong>Anthropic</strong>.</li> <li> <strong>Mistral AI</strong>’s open-source models.</li> <li> <strong>DBRX</strong> by <strong>Databricks</strong>: An open-source LLM².</li> </ol> <h3 id="fine-tuning">Fine Tuning</h3> <p>Fine-tuning is the process of taking a pre-trained model and further training it on a specific task or dataset to enhance its performance for that particular objective. Essentially, fine-tuning enables the adjustment of the parameters of the pre-trained model to adapt it to the intricacies of the target task or domain.</p> <p>Fine-tuning techniques:</p> <ol> <li> <p><strong>LoRA (Low Rank Adaption of LLM)</strong>: LoRA is a fine-tuning technique designed specifically for Large Language Models (LLMs), such as GPT models. It focuses on efficiently adapting pre-trained LLMs to new tasks while mitigating the risk of catastrophic forgetting, which occurs when the model forgets previously learned knowledge while learning new information. LoRA achieves this by introducing low-rank adaptations to the model’s parameters during fine-tuning. By adjusting the rank of parameter matrices in the LLM, LoRA allows for more efficient adaptation to new tasks without significantly increasing computational overhead.</p> </li> <li> <p><strong>QLoRA (Quantized Low Rank Adaption of LLM)</strong>: QLoRA builds upon the principles of LoRA while introducing quantization to further optimize the fine-tuning process. In QLoRA, the low-rank adaptations of LLM parameters are quantized into a discrete set of values. This quantization serves to stabilize training and reduce memory requirements, making fine-tuning more computationally efficient. By combining low-rank adaptation with quantization, QLoRA enables effective fine-tuning of LLMs on diverse tasks with minimal computational overhead.</p> </li> </ol> <p>These techniques represent innovative approaches to fine-tuning LLMs for specific tasks, offering efficient solutions to adapt pre-trained models to new domains while preserving previously learned knowledge.Techniques like LoRA and QLoRA can unlock the full potential of LLMs across a wide range of natural language processing tasks and applications.</p> <p><strong>Fine-tuning and Pretraining a LLM is prohbitively expensive.</strong></p> <hr> <h2 id="prompt-engineering">Prompt Engineering</h2> <p>Prompt engineering involves crafting tailored instructions or queries to guide AI models towards accurate and contextually relevant outputs. For instance, in a conversational AI system designed to provide movie recommendations based on user preferences, a good prompt might include specific criteria such as genre, release year, and preferred actors, such as “Recommend a Nepali movie released in the past five years starring either Bipin Karki or Dayahang Rai.” This nuanced instruction provides the AI model with clear guidance on the user’s preferences, enabling it to generate highly relevant and personalized recommendations, showcasing the intricate nature of prompt engineering in AI applications.</p> <p>The general format:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">Instructions</code>:<code class="language-plaintext highlighter-rouge"> You are an assistant .................</code> </li> <li> <code class="language-plaintext highlighter-rouge">Context</code>: <code class="language-plaintext highlighter-rouge">This is context ......</code> </li> <li> <code class="language-plaintext highlighter-rouge">Question/Query</code>: <code class="language-plaintext highlighter-rouge">Your question .....</code> </li> <li> <code class="language-plaintext highlighter-rouge">Answer</code>: <code class="language-plaintext highlighter-rouge">LLM generated response</code> </li> </ul> <hr> <h2 id="rag-pipeline">RAG Pipeline</h2> <p>RAG Pipeline consists of many components.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rag-pipeline-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rag-pipeline-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rag-pipeline-1400.webp"></source> <img src="/assets/img/rag-pipeline.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <hr> <h2 id="embedding-vectors">Embedding Vectors</h2> <p>In natural language processing (NLP), embedding refers to the process of representing words or tokens as numerical vectors in a continuous vector space. These vectors capture semantic relationships between words, enabling NLP models to understand and process textual data more effectively. BERT (Bidirectional Encoder Representations from Transformers) is a powerful model that generates word embeddings with rich contextual information.</p> <p>Let’s illustrate embedding with the example text <code class="language-plaintext highlighter-rouge">"Kathmandu is city of temple"</code> using BERT:</p> <p>In BERT, each word in the sentence is tokenized and represented as a vector. For instance, the word <code class="language-plaintext highlighter-rouge">"Kathmandu"</code> is tokenized into individual subwords or tokens, such as <code class="language-plaintext highlighter-rouge">["Kath", "##man", "##du"]</code>. Each token is embedded into a high-dimensional vector space.</p> <p>In the case of “Kathmandu is city of temple,” BERT captures the contextual information of each token by considering its surrounding tokens. So, the embedding for <code class="language-plaintext highlighter-rouge">"Kath"</code> might be influenced by the tokens <code class="language-plaintext highlighter-rouge">"is"</code> and <code class="language-plaintext highlighter-rouge">"city,"</code> indicating its context within the sentence.</p> <p>Similarly, the embedding for <code class="language-plaintext highlighter-rouge">"city"</code> would capture its relationship with <code class="language-plaintext highlighter-rouge">"Kathmandu"</code> and <code class="language-plaintext highlighter-rouge">"temple"</code> in the sentence, enabling BERT to understand that <code class="language-plaintext highlighter-rouge">"city"</code> is associated with both <code class="language-plaintext highlighter-rouge">"Kathmandu"</code> and <code class="language-plaintext highlighter-rouge">"temple"</code> in this particular context.</p> <p>By generating embeddings that encode contextual information, BERT enables NLP models to grasp the nuanced meanings of words in different contexts. These embeddings serve as input to downstream tasks, allowing the model to make accurate predictions or generate relevant outputs based on the semantic information captured in the embeddings.</p> <h3 id="sentence-embedding">Sentence Embedding</h3> <p>Sentence embedding refers to the process of representing entire sentences or phrases as numerical vectors in a continuous vector space, capturing their semantic meaning and context. Let’s explore sentence embedding using the example “Kathmandu is city of temple.”</p> <p>In sentence embedding, the sentence “Kathmandu is city of temple” would be tokenized into individual words or subwords and processed to generate a single vector representation that encapsulates its semantic meaning and context.</p> <p>Using a pre-trained language model like BERT or Universal Sentence Encoder, each word or subword in the sentence is converted into a vector representation, and these individual embeddings are combined or aggregated to produce a final vector representation for the entire sentence.</p> <p>For example, the sentence “Kathmandu is city of temple” might be tokenized into the following subwords: <code class="language-plaintext highlighter-rouge">["Kath", "##mandu", "is", "city", "of", "temple"]</code></p> <p>Each of these subwords is then embedded into a high-dimensional vector space. The embedding for the entire sentence is computed by aggregating or combining these individual subword embeddings, typically using techniques like averaging or pooling.</p> <p>The resulting sentence embedding captures the semantic meaning and context of the entire sentence “Kathmandu is city of temple” in a continuous vector representation. This embedding can then be used as input to downstream NLP tasks such as sentiment analysis, text classification, or semantic similarity comparison between sentences, allowing NLP models to make accurate predictions or generate relevant outputs based on the semantic information encoded in the embedding.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sentence_emb-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sentence_emb-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sentence_emb-1400.webp"></source> <img src="/assets/img/sentence_emb.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>How does the model know if two sentence have similar meaning ?</strong></p> <p>One of the popular method to find relationship between sentences (vectors in nutshell) is cosine similarity score. It measures the the angle between two vectors. A small angle results in high score that is also a highly similar vector.</p> <p>Given two n-dimensional vectors of attributes, $A$ and $B$, the cosine similarity, $cos(θ)$, is represented using a dot product and magnitude as</p> \[S_C(A,B)\] \[cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A^2_n} \cdot \sqrt{\sum_{i=1}^n B^2_n}}\] <p>Yet, how to teach BERT to use desired similarity metrics like cosine and assure two sentences produce the similar result?</p> <hr> <h2 id="sentence-bert">Sentence BERT</h2> <p>Sentence BERT (SBERT) is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model specifically designed for generating high-quality sentence embeddings. Unlike traditional BERT, which operates at the word level, SBERT processes entire sentences or phrases to generate embeddings that capture the semantic meaning and context of the input text.</p> <p>SBERT achieves this by fine-tuning the BERT architecture on a variety of sentence-level tasks, such as sentence similarity, paraphrase identification, and natural language inference. During training, SBERT learns to encode the semantic similarity between pairs of sentences, allowing it to generate embeddings that effectively capture the meaning of entire sentences.</p> <p>One key innovation of SBERT is the use of siamese or triplet network architectures, where multiple copies of the BERT model share weights and are trained to optimize a similarity metric between sentence pairs. This encourages SBERT to learn representations that are invariant to certain transformations (e.g., word order or paraphrasing) while emphasizing differences between dissimilar sentences.</p> <p>SBERT embeddings have demonstrated superior performance in various NLP tasks requiring sentence-level understanding, such as semantic textual similarity, text classification, and information retrieval.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sbert-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sbert-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sbert-1400.webp"></source> <img src="/assets/img/sbert.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <hr> <h2 id="vector-database">Vector Database</h2> <p>A vector database is a type of database that stores data in the form of vectors, typically numerical representations of objects or documents in a high-dimensional (predefined) vector space. These databases are designed to efficiently store and retrieve vector data, enabling various applications such as similarity search, recommendation systems, and information retrieval.</p> <p>Suppose we have a collection of documents, each represented as a numerical vector in a high-dimensional space. These vectors capture the semantic meaning and features of the documents.</p> <p>For simplicity, let’s consider a small collection of three documents represented by their vectors:</p> <p>Documents: $A, B, C$ Here, document(s) refer to the chunck of texts from any sources. \(A: [0.2, 0.5, 0.8] \space B: [0.7, 0.3, 0.6] \space C: [0.4, 0.9, 0.1]\)</p> <p>Now, let’s say we have a query document represented by the vector $[0.6, 0.4, 0.7]$. We want to find the most similar document in our collection to this query document.</p> <p>In a vector database, document similarity is computed using techniques such as cosine similarity or Euclidean distance. Let’s use cosine similarity for this example:</p> <p>Cosine similarity between two vectors A and B is calculated as the cosine of the angle between them:</p> \[\text{cosine similarity} = \frac{A \cdot B}{\|A\| \|B\|}\] <p>Where:</p> <ul> <li>$ A \cdot B $ is the dot product of vectors A and B.</li> <li>$ |A| $ and $ |B| $ are the magnitudes of vectors A and B, respectively.</li> </ul> <p>Using cosine similarity, we compute the similarity between the query vector [0.6, 0.4, 0.7] and each document vector in the collection:</p> <ul> <li>Cosine similarity between query and document A: $ \frac{(0.6 \times 0.2) + (0.4 \times 0.5) + (0.7 \times 0.8)}{\sqrt{0.6^2 + 0.4^2 + 0.7^2} \times \sqrt{0.2^2 + 0.5^2 + 0.8^2}} \approx 0.93 $</li> <li>Cosine similarity between query and document B: $ \frac{(0.6 \times 0.7) + (0.4 \times 0.3) + (0.7 \times 0.6)}{\sqrt{0.6^2 + 0.4^2 + 0.7^2} \times \sqrt{0.7^2 + 0.3^2 + 0.6^2}} \approx 0.91 $</li> <li>Cosine similarity between query and document C: $ \frac{(0.6 \times 0.4) + (0.4 \times 0.9) + (0.7 \times 0.1)}{\sqrt{0.6^2 + 0.4^2 + 0.7^2} \times \sqrt{0.4^2 + 0.9^2 + 0.1^2}} \approx 0.81 $</li> </ul> <p>Based on the computed cosine similarities, document A is the most similar to the query document. Therefore, in response to the query, the vector database would return document A as the most similar document to the query document $[0.6, 0.4, 0.7]$.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/v_db-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/v_db-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/v_db-1400.webp"></source> <img src="/assets/img/v_db.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong><em>Amazing!! What if we have extremely large number of documents?</em></strong></p> <p>There are many techniques to address this issue. However, simple one would be to use metadata in each document. It is effective if we are aware of metadata for document clustering that narrows down the computation. In addition, we can employ algorithm such as Hierarchical Navigable Small Worlds (HNSW).</p> <p>Following code returns a vector retriever with only matching filter i.e. metadata.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pinecone example
</span><span class="n">search_kwargs</span> <span class="o">=</span> <span class="p">{</span> <span class="sh">"</span><span class="s">filter</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span> <span class="sh">"</span><span class="s">meta_key</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">metadata_value</span><span class="sh">"</span> <span class="p">}}</span>
<span class="k">return</span> <span class="n">vectore_store</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span>
    <span class="n">search_kwargs</span><span class="o">=</span><span class="n">search_kwargs</span>
<span class="p">)</span>
</code></pre></div></div> <hr> <h2 id="code">Code</h2> <p>A quick reminder, why are we using RAG ? Once or often LLM training and fine-tuning is highly expensive. In addition, some data are highly confidential but need tools like LLM. To address aforementioned issues, we retrieve and augment the LLM capabilities.</p> <p>Lets recall the RAG pipeline section, and use some software engineering skills to develop LLM powered RAG applications. Here, we will just outline the basics of RAG codes using tools such as langchain, SentenceTransformerEmbeddings, OpenAI ChatLLM, and pinecone as a vector database.</p> <p>Following code will demo a pdf based RAG app. Yet, lanchain and tools have magical abilities.</p> <pre><code class="language-env">OPENAI_API_KEY=sk-

PINECONE_API_KEY=
PINECONE_ENV_NAME=
PINECONE_INDEX_NAME=example

</code></pre> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Extra</span>
<span class="k">class</span> <span class="nc">Metadata</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">extra</span><span class="o">=</span><span class="n">Extra</span><span class="p">.</span><span class="n">allow</span><span class="p">):</span>
    <span class="n">conversation_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">user_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">doc_id</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">class</span> <span class="nc">ChatArgs</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">extra</span><span class="o">=</span><span class="n">Extra</span><span class="p">.</span><span class="n">allow</span><span class="p">):</span>
    <span class="n">conversation_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">doc_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">Metadata</span>
    <span class="n">streaming</span><span class="p">:</span> <span class="nb">bool</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>

<span class="k">def</span>  <span class="nf">build_llm</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span> <span class="n">ChatArgs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatOpenAI</span><span class="p">:</span>
    <span class="k">return</span> <span class="nc">ChatOpenAI</span><span class="p">()</span>
</code></pre></div></div> <p>The reason to use SentenceTransformerEmbeddings while using OpenaAI as LLM is OpenaAI has API limiter. If you have pro access to LLMs like OpenAI GPT-*, just replace the follwoing code block with OpenAI embeddings.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">SentenceTransformerEmbeddings</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">SentenceTransformerEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">pinecone</span>
<span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Pinecone</span> 
<span class="kn">from</span> <span class="n">app.chat.embeddings.sentence_transformer</span> <span class="kn">import</span> <span class="n">embeddings</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>

<span class="n">pinecone</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">PINECONE_API_KEY</span><span class="sh">"</span><span class="p">),</span> <span class="n">environment</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">PINECONE_ENV_NAME</span><span class="sh">"</span><span class="p">))</span>

<span class="n">vectore_store</span> <span class="o">=</span> <span class="n">Pinecone</span><span class="p">.</span><span class="nf">from_existing_index</span><span class="p">(</span><span class="n">index_name</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">PINECONE_INDEX_NAME</span><span class="sh">"</span><span class="p">),</span> <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>


<span class="sh">'''</span><span class="s">
We are filtering docs based on metadata such as doc_id. Hence, the document retriever has less document to query against.
</span><span class="sh">'''</span>
<span class="k">def</span> <span class="nf">build_retriever</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span><span class="n">ChatArgs</span><span class="p">):</span>
    <span class="n">search_kwargs</span> <span class="o">=</span> <span class="p">{</span> <span class="sh">"</span><span class="s">filter</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span> <span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">chat_args</span><span class="p">.</span><span class="n">pdf_id</span> <span class="p">}}</span>
    <span class="k">return</span> <span class="n">vectore_store</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span>
        <span class="n">search_kwargs</span><span class="o">=</span><span class="n">search_kwargs</span>
    <span class="p">)</span>

</code></pre></div></div> <p>The following code is just to add history to each chat conversation. [Optional]</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="n">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationBufferMemory</span>
<span class="kn">from</span> <span class="n">langchain.schema</span> <span class="kn">import</span> <span class="n">BaseChatMessageHistory</span>

<span class="kn">from</span> <span class="n">app.web.api</span> <span class="kn">import</span> <span class="n">get_messages_by_conversation_id</span><span class="p">,</span> <span class="n">add_message_to_conversation</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>

<span class="c1">##overriding the default BaseChatMessageHistory
</span><span class="k">class</span> <span class="nc">DemoMessageHistory</span><span class="p">(</span><span class="n">BaseChatMessageHistory</span><span class="p">,</span> <span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">conversation_id</span><span class="p">:</span> <span class="nb">str</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">messages</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">get_messages_by_conversation_id</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">conversation_id</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">add_message</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">add_message_to_conversation</span><span class="p">(</span><span class="n">conversation_id</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">conversation_id</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">message</span><span class="p">.</span><span class="nb">type</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">clear</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    
<span class="k">def</span> <span class="nf">build_memory</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span> <span class="n">ChatArgs</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">ConversationBufferMemory</span><span class="p">(</span>
        <span class="n">chat_memory</span><span class="o">=</span><span class="nc">DemoMessageHistory</span><span class="p">(</span><span class="n">conversation_id</span><span class="o">=</span><span class="n">chat_args</span><span class="p">.</span><span class="n">conversation_id</span><span class="p">),</span>
        <span class="n">return_messages</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">memory_key</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">output_key</span><span class="o">=</span><span class="sh">"</span><span class="s">answer</span><span class="sh">"</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Generate and store embeddings for the given documnent</p> <ul> <li>Extract text from the specified document.</li> <li>Divide the extracted text into manageable chunks.</li> <li>Generate an embedding for each chunk.</li> <li>Persist the generated embeddings to pinecone.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">PyPDFLoader</span>
<span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="n">app.chat.vector_stores.pinecone</span> <span class="kn">import</span> <span class="n">vectore_store</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="k">def</span> <span class="nf">create_embeddings_for_docs</span><span class="p">(</span><span class="n">doc_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">doc_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    @params
    doc_id: The unique identifier for the doc. 
    doc_path: The file path to the doc.
    Usage:
    create_embeddings_for_docs(</span><span class="sh">'</span><span class="s">123456</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">/path/to/pdf</span><span class="sh">'</span><span class="s">)
    </span><span class="sh">"""</span>

    <span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1600</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span><span class="nc">PyPDFLoader</span><span class="p">(</span><span class="n">pdf_path</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load_and_split</span><span class="p">(</span><span class="n">text_splitter</span><span class="o">=</span><span class="n">text_splitter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
        <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span> <span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">:</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">],</span>
            <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">pdf_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">pdf_id</span>
        <span class="p">}</span>
    <span class="n">vectore_store</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

</code></pre></div></div> <p>This piece of code will run when we add new embeddings from new documents to vector database.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">app.web.db.models</span> <span class="kn">import</span> <span class="n">Docs</span>
<span class="kn">from</span> <span class="n">app.web.files</span> <span class="kn">import</span> <span class="n">download</span>
<span class="kn">from</span> <span class="n">app.chat</span> <span class="kn">import</span> <span class="n">create_embeddings_for_pdf</span>

<span class="k">def</span> <span class="nf">process_document</span><span class="p">(</span><span class="n">doc_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">Docs</span><span class="p">.</span><span class="nf">find_by</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">doc_id</span><span class="p">)</span>
    <span class="k">with</span> <span class="nf">download</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span> <span class="k">as</span> <span class="n">doc_path</span><span class="p">:</span>
        <span class="nf">create_embeddings_for_docs</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>

</code></pre></div></div> <p>Now, vector database is ready to query. Let build chat.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">ConversationalRetrievalChain</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>
<span class="kn">from</span> <span class="n">app.chat.vector_stores.pinecone</span> <span class="kn">import</span> <span class="n">build_retriever</span>
<span class="kn">from</span> <span class="n">app.chat.llms.chatopenai</span> <span class="kn">import</span> <span class="n">build_llm</span>
<span class="kn">from</span> <span class="n">app.chat.memories.sql_memory</span> <span class="kn">import</span> <span class="n">build_memory</span>

<span class="k">def</span> <span class="nf">build_chat</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span> <span class="n">ChatArgs</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    @params
    chat_args: ChatArgs object containing
    conversation_id, doc_id, metadata, and streaming flag.
    @return: chain
    Usage:
    chain = build_chat(chat_args)
    </span><span class="sh">"""</span>
    <span class="n">retriever</span> <span class="o">=</span> <span class="nf">build_retriever</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="nf">build_llm</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="nf">build_memory</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ConversationalRetrievalChain</span><span class="p">.</span><span class="nf">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span> <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">)</span>

</code></pre></div></div> <p>running the chat chain:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">input</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Which city has the most temples in Nepal?</span><span class="sh">"</span>
<span class="n">chat_args</span> <span class="o">=</span> <span class="nc">ChatArgs</span><span class="p">(</span>
        <span class="n">conversation_id</span><span class="o">=</span><span class="n">conversation</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
        <span class="n">doc_id</span><span class="o">=</span><span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
        <span class="n">streaming</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">conversation_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">conversation</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">user_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">g</span><span class="p">.</span><span class="n">user</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="n">chat</span> <span class="o">=</span> <span class="nf">build_chat</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div> <hr> <h2 id="conclusion">Conclusion</h2> <p>We have walked through simple RAG application process, why it is required and what it can do? In addition, we explored technologies like large language models, sentence embeddings, sentence BERT, vector database and langchian. We also expolred simple example of RAG pipeline. Yet, tools like langchain, and llamaindex can unlock complex systems with agents.</p> <p>In order to have deep understanding of future apps employing Generative AI, one must understand the inner working atleast of transformer model.</p> <hr> <h2 id="citations">Citations</h2> <p>Thanks to GenAI for content writing.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Tsuyog Basnet. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 13, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>