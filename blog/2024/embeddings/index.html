<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Embedding &amp; Toeknization | Tsuyog Basnet</title> <meta name="author" content="Tsuyog Basnet"> <meta name="description" content="Understanding embedding and toeknization in natural language processing. An implementation of Byte Pair Encoding."> <meta name="keywords" content="Machine-Learninng, Data-Scientist, researcher, NLP, personal-blog"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pvsnp9.github.io/blog/2024/embeddings/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Embedding & Toeknization",
      "description": "Understanding embedding and toeknization in natural language processing. An implementation of Byte Pair Encoding.",
      "published": "May 3, 2024",
      "authors": [
        {
          "author": "Tsuyog Basnet",
          "authorURL": "https://www.linkedin.com/in/tsuyog/",
          "affiliations": [
            {
              "name": "Vector Lab",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Tsuyog Basnet</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Experiments</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Embedding &amp; Toeknization</h1> <p>Understanding embedding and toeknization in natural language processing. An implementation of Byte Pair Encoding.</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#embeddings">Embeddings</a></div> <div><a href="#tokenization">Tokenization</a></div> <ul> <li><a href="#introduction">Introduction</a></li> <li><a href="#text-tokenization">Text tokenization</a></li> <li><a href="#creating-vocabulary">Creating Vocabulary</a></li> <li><a href="#byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#citations">Citations</a></div> </nav> </d-contents> <blockquote> <p>Lets try to communicate with computers. <br></p> </blockquote> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/intro-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/intro-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/intro-1400.webp"></source> <img src="/assets/img/intro.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="NLP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><br></p> <p>A user is curious to know someone’s well being. But, the recipient is perplexed about what was asked!! This is exactly why we need tokenization and embedding to communicate with machines (computers) via natural language.</p> <h2 id="embedding">Embedding</h2> <p>“Embedding is the process of representing words or sentences as numerical vectors in a multi-dimensional space. These numerical representations capture semantic meaning and relationships, enabling machines to understand and process natural language more effectively in various tasks.”</p> <p>Imagine you have a dictionary, and instead of definitions, each word has a unique set of numbers associated with it. These numbers are like coordinates in a multi-dimensional space. Each word’s set of numbers represents its meaning and context in relation to other words.</p> <p>For example, in this numerical space, the word <code class="language-plaintext highlighter-rouge">"Nepal"</code> might be represented by the numbers <code class="language-plaintext highlighter-rouge">(0.3, -0.1, 0.5)</code>, while <code class="language-plaintext highlighter-rouge">"Everest"</code> might be represented by <code class="language-plaintext highlighter-rouge">(0.2, -0.3, 0.6)</code>. Notice how similar words like “Nepal” and “Everest” have similar sets of numbers, indicating their semantic similarity.</p> <p>Now, these numerical representations, or embeddings, are incredibly useful for machines. They allow computers to understand the meaning of words based on their context and relationships with other words. This understanding is crucial for tasks like sentiment analysis, machine translation, text classification and text generation.</p> <p>The embedding technique is not only limited to langauge/text but also is employed in audio and video embeddings.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/embeddings-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/embeddings-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/embeddings-1400.webp"></source> <img src="/assets/img/embeddings.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Embeddings" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>There are several embedding models such as WOrd2Vec, GloVe(Global Vectors for Word Representation), FastText, BERT (Bidirectional Encoder Representations from Transformers), ELMo (Embeddings from Language Models), and etc.</p> <p><strong>If every word has single unique vector representation, which is good. But same word (token) has different meaning based on the sentence.</strong></p> <p>Here is an example: <code class="language-plaintext highlighter-rouge">"I traveled to Nepal to explore the breathtaking Himalayan mountains."</code> and <code class="language-plaintext highlighter-rouge">"One of my friend's name is X Nepal."</code>. Words often have multiple senses or meanings, and their interpretation can change based on the words. First example referes to country due to context like <code class="language-plaintext highlighter-rouge">"to explore," "breathtaking," and "Himalayan mountains"</code>, and second represents a noun with context of <code class="language-plaintext highlighter-rouge">"friend", "name"</code>. Contextualized embedding models like BERT and ELMo are designed to capture these nuances by considering the surrounding words when generating word embeddings, allowing for more accurate representations of words in different contexts.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/embedding_viz-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/embedding_viz-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/embedding_viz-1400.webp"></source> <img src="/assets/img/embedding_viz.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Embedding visaulization" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Visaulization of two dimensional embedding vectors. However, the embedding dimensions are different according to model. Similar items are in same cluster, in another words they have shorter distance. </div> <hr> <h2 id="tokenization">Tokenization</h2> <p>Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, symbols, or any meaningful units, which are then used for further analysis in natural language processing (NLP) tasks.</p> <h3 id="introduction">Introduction</h3> <p>Imagine you have a long paragraph of text. Tokenization is like chopping it into smaller, manageable pieces, kind of like slicing a cake into individual slices. But instead of using a knife, we use rules to decide where to make these cuts.</p> <p>Each slice we get after chopping is called a “token”. Tokens can be words, but they can also be punctuation marks, numbers, or even emojis! Essentially, anything that makes sense as a unit in the text.</p> <p>For example, in the sentence “I love natural language processing!”, the tokens would be “I”, “love”, “natural”, “language”, “processing”, and “!”. Each of these is a separate token.</p> <p>Once we’ve chopped up our text into tokens, we can do all sorts of cool things with them, like counting how many times each word appears, figuring out the meaning of a sentence, or even teaching computers to understand and generate human-like language!</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/tokenizer-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/tokenizer-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/tokenizer-1400.webp"></source> <img src="/assets/img/tokenizer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Tokenization process" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Visaulization of tokenization in LLM. First, we split the input text into individual tokens that are either words or characters, and encode using tools like tiktoken or custom encoder.</div> <p>Here is sample code split the input sentence:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="n">input_text</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">"</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">([,.?_!</span><span class="sh">"</span><span class="s">()\']|--|\s)</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
<span class="p">[</span><span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">traveled</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Nepal</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">explore</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">breathtaking</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Himalayan</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mountains</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p>We must employ complex regex patterns extract useful text from dataset.</p> <h3 id="creating-vocabulary">Creating Vocabulary</h3> <p>Vocabulary refers to a set of unique words that occur in a given corpus or dataset. It represents the entire range of words used in the text data being analyzed. In python dialect, it is a dictionary for all possible words in corpus mapped to numerical IDs.</p> <p>To understand, imagine you have a large collection of books. Each book contains many different words, right? Now, if you were to make a list of all the unique words that appear across all the books, that list would be your vocabulary.</p> <p>So, essentially, a vocabulary is like a dictionary of words used in a specific context or dataset. It includes every distinct word found in the text data, without repetition.</p> <p>For example, if you were analyzing a set of news articles, your vocabulary might include words like “politics,” “election,” “economy,” and so on.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/vocab-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/vocab-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/vocab-1400.webp"></source> <img src="/assets/img/vocab.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="vocabulary illustration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Illustration for creating tokenizer vocabulary.The values are taken from tiktoken encoding.</div> <p><strong>Qestion: any number and any range would do the job?</strong> Yes, the sole purpose of vocabulary is lookup, string to id and vice versa. In case of size, it is task dependent.</p> <p>Following code will demonstrate a simple implementation (reference to previous code example):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique_words</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">)))</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">unique_words</span><span class="p">)}</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span> <span class="nf">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

<span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">Himalayan</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">Nepal</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">breathtaking</span><span class="sh">'</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">explore</span><span class="sh">'</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">mountains</span><span class="sh">'</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">traveled</span><span class="sh">'</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
</code></pre></div></div> <p>Using the prior knowledge, lets create a very simple tokenizer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ToeknizerV1</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">:</span><span class="nb">dict</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="o">=</span>  <span class="n">vocab</span>
        <span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="c1"># use regex to tokenize the input text
</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">([,.?_!</span><span class="sh">"</span><span class="s">()\']|--|\s)</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
        
        <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">idxs</span>
    
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\s+([,.?!</span><span class="sh">"</span><span class="s">()\'])</span><span class="sh">'</span><span class="p">,</span> <span class="sa">r</span><span class="sh">'</span><span class="s">\1</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">text</span>
</code></pre></div></div> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/enc-dec-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/enc-dec-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/enc-dec-1400.webp"></source> <img src="/assets/img/enc-dec.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="vocabulary illustration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Illustration for encoding-decoding in tokenizer.</div> <p>Lets see it in action</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer_v1</span> <span class="o">=</span> <span class="nc">ToeknizerV1</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">idxs</span> <span class="o">=</span> <span class="n">tokenizer_v1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">tokenizer_va</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">idxs</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p>The encoder output is <code class="language-plaintext highlighter-rouge">[IDS]</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>The decoder outpus is <code class="language-plaintext highlighter-rouge">'str'</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">'</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">'</span>
</code></pre></div></div> <p>Try this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">try_input</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Kathmandu is capital city of Nepal</span><span class="sh">"</span>
<span class="n">tokenizer_v1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
</code></pre></div></div> <p><strong>oops…</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">KeyError</span><span class="p">:</span> <span class="sh">'</span><span class="s">Kathmandu</span><span class="sh">'</span>
</code></pre></div></div> <h4 id="special-tokens">Special tokens</h4> <p>The token <code class="language-plaintext highlighter-rouge">'Kathmandu'</code> does not exists in our vocab. Hence, the vocab should be rich enough to afford all tokens. However, a quick fix is to add special tokens such as <code class="language-plaintext highlighter-rouge">&lt;|unk|&gt;</code> , <code class="language-plaintext highlighter-rouge">&lt;|sos|&gt;</code>, and <code class="language-plaintext highlighter-rouge">&lt;|eos|&gt;</code>. Lets extend the unique words and vocabulary by adding special tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique_words</span><span class="p">.</span><span class="nf">extend</span><span class="p">([</span><span class="sh">"</span><span class="s">&lt;|unk|&gt;</span><span class="sh">"</span> <span class="p">,</span> <span class="sh">"</span><span class="s">&lt;|sos|&gt;</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">&lt;|eos|&gt;</span><span class="sh">"</span><span class="p">])</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">unique_words</span><span class="p">)}</span>
<span class="nf">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</code></pre></div></div> <p>New vocab list:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">Himalayan</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">Nepal</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">breathtaking</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">explore</span><span class="sh">'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">mountains</span><span class="sh">'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">traveled</span><span class="sh">'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">&lt;|unk|&gt;</span><span class="sh">'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">&lt;|sos|&gt;</span><span class="sh">'</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">&lt;|eos|&gt;</span><span class="sh">'</span><span class="p">:</span> <span class="mi">12</span><span class="p">}</span>
</code></pre></div></div> <p>wait wait…. we still need to modify <code class="language-plaintext highlighter-rouge">Tokenizer</code> encoding method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ToeknizerV2</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">:</span><span class="nb">dict</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="o">=</span>  <span class="n">vocab</span>
        <span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="c1"># use regex to tokenize the input text
</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">([,.?_!</span><span class="sh">"</span><span class="s">()\']|--|\s)</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">if</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span> <span class="k">else</span> <span class="sh">"</span><span class="s">&lt;|unk|&gt;</span><span class="sh">"</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        
        <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">idxs</span>
    
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\s+([,.?!</span><span class="sh">"</span><span class="s">()\'])</span><span class="sh">'</span><span class="p">,</span> <span class="sa">r</span><span class="sh">'</span><span class="s">\1</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">text</span>
</code></pre></div></div> <p>Test out new tokenizer with <code class="language-plaintext highlighter-rouge">out-of-bag/vocabulary (OOB)</code> text.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">try_input</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Kathmandu is capital city of Nepal.&lt;|eos|&gt;</span><span class="sh">"</span>
<span class="n">t_v2</span> <span class="o">=</span> <span class="nc">ToeknizerV2</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">t_v2</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">try_input</span><span class="p">)</span>
</code></pre></div></div> <p>The encoder output</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t_v2</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">t_v2</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">try_input</span><span class="p">))</span>
</code></pre></div></div> <p>the decoder output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">'</span><span class="s">&lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; Nepal. &lt;|eos|&gt;</span><span class="sh">'</span>
</code></pre></div></div> <p>umm….. we solve the error but it is not going be useful during model training. In the following section we will develop BPE to address such issue.</p> <hr> <h3 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h3> <p>Byte Pair Encoding (BPE) is a subword tokenization technique used in natural language processing (NLP) to break down words into smaller, meaningful units called subword tokens. This technique is widely used in tasks such as machine translation, text generation, and language modeling.</p> <p>Similar tokenization techniques include WordPiece and SentencePiece, which are also subword tokenization methods that segment words into smaller units. These techniques are particularly useful for handling out-of-vocabulary words, morphologically rich languages, and reducing the size of the vocabulary in NLP models.</p> <p>Imagine you have a large collection of words in a language, like English. Some words are very common, like “the” or “and,” while others are rare or even completely new. Byte Pair Encoding helps in representing all these words by breaking them down into smaller parts called subword tokens.</p> <h4 id="algorithm">Algorithm</h4> <ul> <li> <strong>Step 1: Vocabulary initialization</strong> - Start with a vocabulary containing all unique characters present in the training data.</li> <li> <strong>Step 2: Merging:</strong>- Iteratively merge the most frequent pair of adjacent subword tokens in the vocabulary. Repeat this process for a specified number of iterations or until a certain vocabulary size is reached.</li> <li> <strong>Step 3: Vocabulary Expansion</strong> - After each merge, update the vocabulary to include the newly created subword tokens.</li> <li> <strong>Step 4: Tokenization</strong> - Segment input text into subword tokens based on the learned vocabulary. Replace words not in the vocabulary with a special token, such as <code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code> for unknown.</li> </ul> <p>By iteratively applying these steps, Byte Pair Encoding effectively captures both frequent and rare patterns in the data, leading to compact and efficient representations for a wide range of words in the language.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bpe-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bpe-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bpe-1400.webp"></source> <img src="/assets/img/bpe.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="vocabulary illustration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Illustration for byte pair encoding tokenizer. It breaks down unknown words into subwords and characters.</div> <p>Let’s use Byte Pair Encoding (BPE) to tokenize the input text. We’ll start by initializing the vocabulary with individual unique characters, then iteratively merge the most frequent pairs of adjacent subword tokens. For this example, let’s perform two iterations of BPE:</p> <p><strong>Initialization:</strong></p> <ul> <li>Vocabulary: <code class="language-plaintext highlighter-rouge">{"I", " ", "t", "r", "a", "v", "e", "l", "d", "N", "p", "l", "o", "x", "u", "h", "b", "m", "i", "n", "s", "."}</code> </li> </ul> <p><strong>Iteration 1:</strong></p> <ul> <li>Most frequent pair: <code class="language-plaintext highlighter-rouge">("t", "o")</code> </li> <li>Merge <code class="language-plaintext highlighter-rouge">"t"</code> and <code class="language-plaintext highlighter-rouge">"o"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"to"</code>.</li> <li>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"I", " ", "to", "r", "a", "v", "e", "l", "d", "N", "p", "l", "o", "x", "u", "h", "b", "m", "i", "n", "s", "."}</code> </li> </ul> <p><strong>Iteration 2:</strong></p> <ul> <li>Most frequent pair: <code class="language-plaintext highlighter-rouge">("e", " ")</code> </li> <li>Merge <code class="language-plaintext highlighter-rouge">"e"</code> and <code class="language-plaintext highlighter-rouge">" "</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"e "</code>.</li> <li>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"I", " ", "to", "r", "a", "v", "e ", "l", "d", "N", "p", "l", "o", "x", "u", "h", "b", "m", "i", "n", "s", "."}</code> </li> </ul> <p>Now, let’s tokenize the text using this vocabulary:</p> <p><strong>Text tokenization:</strong> <code class="language-plaintext highlighter-rouge">["I", " ", "trav", "eled", " ", "to", " ", "N", "ep", "al", " ", "to", " ", "exp", "lor", "e", " ", "the", " ", "br", "eat", "htaking", " ", "H", "ima", "lay", "an", " ", "m", "oun", "tains", "."]</code></p> <p>This tokenization captures both common patterns like <code class="language-plaintext highlighter-rouge">"to"</code> and <code class="language-plaintext highlighter-rouge">"e "</code> as well as less frequent patterns like “trav” and “moun”, resulting in a more compact representation of the text compared to simple word tokenization.</p> <h4 id="bpe-implmentation">BPE Implmentation</h4> <p>We are going to use unicode characters to make more generic tokenizer. Lets create dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">regex</span> <span class="k">as</span> <span class="n">re</span>

<span class="n">raw_demo_text</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">😄 0123456789 Webb telescope probably didn</span><span class="sh">'</span><span class="s">t find life on an exoplanet -- yet Claims of biosignature gas detection were premature. Recent reports of NASA</span><span class="sh">'</span><span class="s">s James Webb Space Telescope finding signs of life on a distant planet understandably sparked excitement. A new study challenges this finding, but also outlines how the telescope might verify the presence of the life-produced gas. Source:University of California - Riverside. २० वैशाख, काठमाडौं । वेस्टइन्डिज ‘ए’ले चौथो टी २० खेलमा घरेलु टोली नेपाललाई २८ रनले हराउँदै एक खेल अगावै सिरिज जितेको छ ।पाँच खेलको सिरिजमा वेस्टइन्डिज ३–१ ले अघि छ । पहिलो खेल हारे पनि त्यसपछि लगातार तीन खेलमा जितका साथ सिरिज जितेको हो । ११० रनको लक्ष्य पछ्याएको नेपाल २० ओभरमा १८१ रनमा अल आउट भएको छ । नेपालका कप्तान रोहित पौडेलले ४७ बलमा ७ चौका र ५ छक्कासहित सर्वाधिक ८२ रन बनाए पनि जित्नलाई पर्याप्त भएन । रोहितले पहिलो खेलमा ११२, दोस्रो खेलमा ७१ रन र तेस्रो खेलमा विश्राम गरेका थिए । रोहित बाहेक आजको खेलमा पनि अन्य ब्याटरले राम्रो प्रदर्शन गर्न सकेनन् । स्पिनर हेडन वाल्स जुनियरको १४औं ओभरको तेस्रो बलमा रोहित लङअफमा म्याथ्यु फर्डबाट क्याच आउट भएपछि शतकको अवसर गुमाएका थिए । ओपनर आसिफ शेख शून्य र कुशल भुर्तेल १ रनमा आउट भएका थिए । कुशल मल्ल ४, सन्दीप जोरा, दीपेन्द्रसिंह ऐरी र गुलशन झा १९–१९ रनमा आउट भए । अभिनाश बोहरा ८ बलमा २ चौका र १ छक्कासहित १७ र सोमपाल कामी १० रनमा आउट भए । Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪!</span><span class="sh">'''</span>

</code></pre></div></div> <p>In this implementation, we will be using <code class="language-plaintext highlighter-rouge">utf-8</code> encode for some advatanges. lets map all <code class="language-plaintext highlighter-rouge">str</code> to <code class="language-plaintext highlighter-rouge">UTF-8</code> format and then convert it into a list of integers representing the bytes of the UTF-8 encoded string.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">raw_demo_text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">====================================</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Text length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">raw_demo_text</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">raw_demo_text</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">====================================</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="mi">50</span><span class="p">])</span>

</code></pre></div></div> <p>Output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">====================================</span>
<span class="n">Text</span> <span class="n">length</span><span class="p">:</span> <span class="mi">1349</span>

<span class="err">😄</span> <span class="mi">0123456789</span> <span class="n">Webb</span> <span class="n">telescope</span> <span class="n">probably</span> <span class="n">didn</span><span class="sh">'</span><span class="s">t find life on an exoplanet -- yet Claims of biosignature 
====================================
Token length: 2846
[240, 159, 152, 132, 32, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 32, 87, 101, 98, 98, 32, 116, 101, 108, 101, 115, 99, 111, 112, 101, 32, 112, 114, 111, 98, 97, 98, 108, 121, 32, 100, 105, 100, 110, 39, 116, 32, 102, 105, 110]

</span></code></pre></div></div> <p>Lets create a function that creates a consecutive pairs of chracters present in the dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    @params: list[int]
    returns: {(pairs: tuples): int(occurance) }
    </span><span class="sh">'''</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">counts</span>
</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">)</span>
</code></pre></div></div> <p>output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{(</span><span class="mi">240</span><span class="p">,</span> <span class="mi">159</span><span class="p">):</span> <span class="mi">15</span><span class="p">,</span> <span class="p">(</span><span class="mi">159</span><span class="p">,</span> <span class="mi">152</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">152</span><span class="p">,</span> <span class="mi">132</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">132</span><span class="p">,</span> <span class="mi">32</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">48</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">49</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">49</span><span class="p">,</span> <span class="mi">50</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">52</span><span class="p">,</span> <span class="mi">53</span><span class="p">):</span> <span class="mi">1</span><span class="p">}</span>
</code></pre></div></div> <p>lets sort them by value (occurance) and pick top k.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(((</span><span class="n">v</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">pair_stats</span><span class="p">.</span><span class="nf">items</span><span class="p">()),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="p">[(</span><span class="mi">531</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">164</span><span class="p">)),</span> <span class="p">(</span><span class="mi">177</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">165</span><span class="p">)),</span> <span class="p">(</span><span class="mi">171</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span> <span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="mi">190</span><span class="p">)),</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="mi">176</span><span class="p">)),</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="p">(</span><span class="mi">176</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="mi">178</span><span class="p">))]</span>
</code></pre></div></div> <p>Lets understand the token merge and vocabulary extension in BPE. For instance,</p> <p>vacob: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d"}</code></p> <p>input_text (dataset): <code class="language-plaintext highlighter-rouge">"aaabdaaabac"</code></p> <p>pair tokens: <code class="language-plaintext highlighter-rouge">{"aa": 3, "ab": 1, "bd": 1, "da": 2, "ac": 1}</code></p> <p>Iteration 1:</p> <p>Most frequent pair: “aa” (frequency = 3)</p> <p>Merge <code class="language-plaintext highlighter-rouge">"a"</code> and <code class="language-plaintext highlighter-rouge">"a"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"Z"</code>.</p> <p>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d", "Z"}</code>, and updated text <code class="language-plaintext highlighter-rouge">ZabdZabac</code></p> <p>new pair tokens: <code class="language-plaintext highlighter-rouge">{"ab":2, "bd":1,"dZ":1, "ac":1 }</code></p> <p>Iteration 2:</p> <p>Most frequent pair: “ab” (frequency = 2)</p> <p>Merge <code class="language-plaintext highlighter-rouge">"a"</code> and <code class="language-plaintext highlighter-rouge">"b"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"Y"</code>.</p> <p>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d", "Z", "Y"}</code>, and updated text <code class="language-plaintext highlighter-rouge">ZYdZYac</code></p> <p>new pair tokens: <code class="language-plaintext highlighter-rouge">{"ZY":2, "Yd":1, "ac":1}</code></p> <p>Iteration 3:</p> <p>Most frequent pair: <code class="language-plaintext highlighter-rouge">"ZY"</code> (frequency = 2)</p> <p>Merge <code class="language-plaintext highlighter-rouge">"Z"</code> and <code class="language-plaintext highlighter-rouge">"Y"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"X"</code>.</p> <p>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d", "Z", "Y", "X"}</code>, and updated text <code class="language-plaintext highlighter-rouge">XdXac</code></p> <p><strong>Here we have compressed the sequence length from 11 to 5, but increased the vocab size from 4 to 7.</strong></p> <p>Lets see this in code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="c1"># list of ints (ids), replace all consecutive occurances of pair with new token idx 
</span>    <span class="n">newids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span><span class="nf">len </span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
        <span class="c1"># if we are not at very last position AND the pair matches, replace it 
</span>        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">newids</span>

</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">demo: </span><span class="si">{</span><span class="nf">merge</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="mi">999</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>output: <code class="language-plaintext highlighter-rouge">demo: [1, 2, 999, 5]</code> pair <code class="language-plaintext highlighter-rouge">(2,3)</code> is replaced with <code class="language-plaintext highlighter-rouge">999</code>.</p> <p>Here, we create the vocab size of 333 (a hyperparameter), and merge tokens. Here, we are using utf-8 that is 256 tokens of raw bytes. The merged token will have IDs from 256. Following codes runs 77 (333-256) iterations and creates new merged tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">voacab_size</span> <span class="o">=</span> <span class="mi">333</span>
<span class="n">required_merges</span> <span class="o">=</span> <span class="n">voacab_size</span> <span class="o">-</span> <span class="mi">256</span>
<span class="c1"># new copy of ids 
</span><span class="n">ids</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">required_merges</span><span class="p">):</span>
    <span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pair_stats</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">+</span> <span class="n">i</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">merging </span><span class="si">{</span><span class="n">pair</span><span class="si">}</span><span class="s"> into new token </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="nf">merge</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>merging (224, 164) into new token 256
merging (224, 165) into new token 257
merging (32, 256) into new token 258
merging (256, 190) into new token 259
merging (257, 141) into new token 260
merging (260, 256) into new token 261
merging (256, 191) into new token 262
.....................................
.....................................
merging (116, 101) into new token 332
</code></pre></div></div> <p>Compression result:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">toekn length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="s">, ids length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s">compression ratio: </span><span class="si">{</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">))</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">X</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">toekn length: 2846, ids length: 1094 compression ratio: 2.60X</code>. We have compressed the text by 2.6 X.</p> <p>create the vocab. it stores the byte information for given IDs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span> <span class="n">idx</span><span class="p">:</span> <span class="nf">bytes</span><span class="p">([</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span> <span class="p">}</span>
<span class="nf">for </span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">),</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">p0</span><span class="p">]</span> <span class="o">+</span> <span class="n">vocab</span><span class="p">[</span><span class="n">p1</span><span class="p">]</span>
</code></pre></div></div> <p>Here, we will create a encoder and decoder block. The encoder returns the token IDs <code class="language-plaintext highlighter-rouge">list[int]</code> for a given text, and decoder returns the text for given <code class="language-plaintext highlighter-rouge">list[int]</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># given a text, return token (list of integers)
</span><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="c1"># extract raw bytes, converted to list of integers
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">))</span>
    <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">pair</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">merges</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">pair</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">merges</span><span class="p">:</span> <span class="k">break</span> <span class="c1"># nothing can be merged
</span>        <span class="n">idx</span> <span class="o">=</span> <span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="nf">merge</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokens</span>    

<span class="c1"># given list of integers, returns python string
</span><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="sa">b</span><span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="sh">"</span><span class="s">replace</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span>
    
</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">enc</span> <span class="o">=</span> <span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">"</span><span class="p">)</span>
<span class="n">dec</span> <span class="o">=</span> <span class="nf">decode</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">encoded: </span><span class="si">{</span><span class="n">enc</span><span class="si">}</span><span class="se">\n</span><span class="s">decoded:</span><span class="si">{</span><span class="n">dec</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">encoded: [73, 32, 116, 114, 97, 118, 101, 108, 101, 100, 32, 116, 111, 32, 78, 101, 112, 97, 108, 32, 116, 111, 32, 101, 120, 112, 108, 111, 114, 277, 116, 104, 277, 98, 114, 101, 97, 116, 104, 116, 97, 107, 304, 103, 32, 72, 105, 109, 97, 108, 97, 121, 317, 32, 109, 111, 117, 110, 116, 97, 304, 115, 46]</code></p> <p><code class="language-plaintext highlighter-rouge">decoded:I traveled to Nepal to explore the breathtaking Himalayan mountains.</code></p> <p>Now, lets create simple BPE version 1 tokenizer. This tokenizer will first take dataset i.e. text to construct vocabulary and merges with highly frequent pairs until desired vocab size. The instance then can encode and decode the test and IDs repectively.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BPEV1</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span><span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__parse_text</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">__create_vocab</span><span class="p">()</span>
        <span class="k">del</span> <span class="n">self</span><span class="p">.</span><span class="n">tokens</span>

    
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__parse_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">pair_stats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__get_pair_data</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">pair</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span> <span class="k">break</span> <span class="c1"># nothing can be merged
</span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__merge</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tokens</span>  
        
        
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="sa">b</span><span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">)</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="sh">"</span><span class="s">replace</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">text</span>
    
    
    <span class="k">def</span> <span class="nf">__parse_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">__get_pair_data</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
        <span class="sh">'''</span><span class="s">
        @params: list[int]
        returns: {(pairs: tuples): int(occurance) }
        </span><span class="sh">'''</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">counts</span>

    <span class="k">def</span> <span class="nf">__merge</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">pair</span><span class="p">:</span><span class="nb">tuple</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
        <span class="n">newids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span><span class="nf">len </span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
            <span class="c1"># if we are not at very last position AND the pair matches, replace it 
</span>            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">newids</span>
    <span class="k">def</span> <span class="nf">__create_vocab</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">new_vocabs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">256</span>
        <span class="k">assert</span> <span class="n">new_vocabs</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">vocab size must be greater than 256</span><span class="sh">"</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span> <span class="n">idx</span><span class="p">:</span> <span class="nf">bytes</span><span class="p">([</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)}</span>
        <span class="c1"># merging
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">new_vocabs</span><span class="p">):</span>
            <span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">)</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pair_stats</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">+</span> <span class="n">i</span>
            <span class="n">self</span><span class="p">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__merge</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        
        <span class="nf">for </span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">),</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">p0</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">p1</span><span class="p">]</span>
    
</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">bpev1</span> <span class="o">=</span> <span class="nc">BPEV1</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">raw_demo_text</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">333</span><span class="p">)</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">"</span><span class="p">)</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">encoded: </span><span class="si">{</span><span class="n">enc</span><span class="si">}</span><span class="se">\n</span><span class="s">decoded:</span><span class="si">{</span><span class="n">dec</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p>Output: look at previous section.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># validation
</span><span class="n">test_txt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s"> म रोयल नेपाल गल्फ क्लबमा खेल्थें । उहाँले सोही समय गल्फ खेल्न सुरु गर्नु भएको हो । उहाँले मलाई सानो भाइको रुपमा माया गर्नुहुन्थ्यो ।</span><span class="sh">'''</span>
<span class="n">val_txt</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">bpev1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">txt</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">test_txt</span> <span class="o">==</span> <span class="n">val_txt</span><span class="p">)</span>
</code></pre></div></div> <p>output: <code class="language-plaintext highlighter-rouge">True</code></p> <p>Finally, we have implemented the Byte pair encoder and decoder on small text dataset. However, this can be extended to larger dataset as well. It is robust and capable to accomodate most of the unseen texts.</p> <p>Take a look at the following, example python script:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
import tiktoken
sample_text = </span><span class="sh">"</span><span class="s">आइएमइ अध्यक्ष चन्द्र प्रसाद ढकालले बाह्रखरीसँग गरेको कुराकानीमा आधारित</span><span class="sh">"</span><span class="s">
#GPT2 
enc = tiktoken.get_encoding(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="s">)
print(enc.encode(sample_text))

#GPT4 (merge space)
enc = tiktoken.get_encoding(</span><span class="sh">"</span><span class="s">cl100k_base</span><span class="sh">"</span><span class="s">)
gpt4tokens = enc.encode(sample_text)
print(gpt4tokens)</span><span class="sh">'''</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">tt</span><span class="p">)</span>
<span class="n">bpev1</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>

</code></pre></div></div> <p>Output: <code class="language-plaintext highlighter-rouge">'\nimport tiktoken\nsample_text = "आइएमइ अध्यक्ष चन्द्र प्रसाद ढकालले बाह्रखरीसँग गरेको कुराकानीमा आधारित"\n#GPT2 \nenc = tiktoken.get_encoding("gpt2")\nprint(enc.encode(sample_text))\n\n#GPT4 (merge space)\nenc = tiktoken.get_encoding("cl100k_base")\ngpt4tokens = enc.encode(sample_text)\nprint(gpt4tokens)'</code></p> <p>Aweesome !!</p> <h2 id="conclusion">Conclusion</h2> <p>We have discussed embedding and tkonization techniques primarily what, why, and how? Every NLP tasks need to be tokenized and embedded in order to train, fine-tune, and generate text from deep learning models such as BERT, GPT, llama, mistral and etc. Embedding is extended to encode any non numerical data such as signals and vision.</p> <hr> <h2 id="citations">Citations</h2> <ul> <li>Thanks to GenAI for content writing.</li> <li>A <a href="https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=6359s" rel="external nofollow noopener" target="_blank">tutorial</a> by A. Karpathy.</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Tsuyog Basnet. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 23, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>