<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://pvsnp9.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pvsnp9.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-04T19:18:58+00:00</updated><id>https://pvsnp9.github.io/feed.xml</id><title type="html">Tsuyog Basnet</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Embedding &amp;amp; Toeknization</title><link href="https://pvsnp9.github.io/blog/2024/embeddings/" rel="alternate" type="text/html" title="Embedding &amp;amp; Toeknization"/><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/embeddings</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/embeddings/"><![CDATA[<blockquote> <p>Lets try to communicate with computers. <br/></p> </blockquote> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/intro-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/intro-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/intro-1400.webp"/> <img src="/assets/img/intro.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="NLP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><br/></p> <p>A user is curious to know someone‚Äôs well being. But, the recipient is perplexed about what was asked!! This is exactly why we need tokenization and embedding to communicate with machines (computers) via natural language.</p> <h2 id="embedding">Embedding</h2> <p>‚ÄúEmbedding is the process of representing words or sentences as numerical vectors in a multi-dimensional space. These numerical representations capture semantic meaning and relationships, enabling machines to understand and process natural language more effectively in various tasks.‚Äù</p> <p>Imagine you have a dictionary, and instead of definitions, each word has a unique set of numbers associated with it. These numbers are like coordinates in a multi-dimensional space. Each word‚Äôs set of numbers represents its meaning and context in relation to other words.</p> <p>For example, in this numerical space, the word <code class="language-plaintext highlighter-rouge">"Nepal"</code> might be represented by the numbers <code class="language-plaintext highlighter-rouge">(0.3, -0.1, 0.5)</code>, while <code class="language-plaintext highlighter-rouge">"Everest"</code> might be represented by <code class="language-plaintext highlighter-rouge">(0.2, -0.3, 0.6)</code>. Notice how similar words like ‚ÄúNepal‚Äù and ‚ÄúEverest‚Äù have similar sets of numbers, indicating their semantic similarity.</p> <p>Now, these numerical representations, or embeddings, are incredibly useful for machines. They allow computers to understand the meaning of words based on their context and relationships with other words. This understanding is crucial for tasks like sentiment analysis, machine translation, text classification and text generation.</p> <p>The embedding technique is not only limited to langauge/text but also is employed in audio and video embeddings.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/embeddings-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/embeddings-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/embeddings-1400.webp"/> <img src="/assets/img/embeddings.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Embeddings" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>There are several embedding models such as WOrd2Vec, GloVe(Global Vectors for Word Representation), FastText, BERT (Bidirectional Encoder Representations from Transformers), ELMo (Embeddings from Language Models), and etc.</p> <p><strong>If every word has single unique vector representation, which is good. But same word (token) has different meaning based on the sentence.</strong></p> <p>Here is an example: <code class="language-plaintext highlighter-rouge">"I traveled to Nepal to explore the breathtaking Himalayan mountains."</code> and <code class="language-plaintext highlighter-rouge">"One of my friend's name is X Nepal."</code>. Words often have multiple senses or meanings, and their interpretation can change based on the words. First example referes to country due to context like <code class="language-plaintext highlighter-rouge">"to explore," "breathtaking," and "Himalayan mountains"</code>, and second represents a noun with context of <code class="language-plaintext highlighter-rouge">"friend", "name"</code>. Contextualized embedding models like BERT and ELMo are designed to capture these nuances by considering the surrounding words when generating word embeddings, allowing for more accurate representations of words in different contexts.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/embedding_viz-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/embedding_viz-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/embedding_viz-1400.webp"/> <img src="/assets/img/embedding_viz.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Embedding visaulization" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Visaulization of two dimensional embedding vectors. However, the embedding dimensions are different according to model. Similar items are in same cluster, in another words they have shorter distance. </div> <hr/> <h2 id="tokenization">Tokenization</h2> <p>Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, symbols, or any meaningful units, which are then used for further analysis in natural language processing (NLP) tasks.</p> <h3 id="introduction">Introduction</h3> <p>Imagine you have a long paragraph of text. Tokenization is like chopping it into smaller, manageable pieces, kind of like slicing a cake into individual slices. But instead of using a knife, we use rules to decide where to make these cuts.</p> <p>Each slice we get after chopping is called a ‚Äútoken‚Äù. Tokens can be words, but they can also be punctuation marks, numbers, or even emojis! Essentially, anything that makes sense as a unit in the text.</p> <p>For example, in the sentence ‚ÄúI love natural language processing!‚Äù, the tokens would be ‚ÄúI‚Äù, ‚Äúlove‚Äù, ‚Äúnatural‚Äù, ‚Äúlanguage‚Äù, ‚Äúprocessing‚Äù, and ‚Äú!‚Äù. Each of these is a separate token.</p> <p>Once we‚Äôve chopped up our text into tokens, we can do all sorts of cool things with them, like counting how many times each word appears, figuring out the meaning of a sentence, or even teaching computers to understand and generate human-like language!</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/tokenizer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/tokenizer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/tokenizer-1400.webp"/> <img src="/assets/img/tokenizer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Tokenization process" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Visaulization of tokenization in LLM. First, we split the input text into individual tokens that are either words or characters, and encode using tools like tiktoken or custom encoder.</div> <p>Here is sample code split the input sentence:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="n">input_text</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">"</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">([,.?_!</span><span class="sh">"</span><span class="s">()\']|--|\s)</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
<span class="p">[</span><span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">traveled</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Nepal</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">explore</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">breathtaking</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Himalayan</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mountains</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p>We must employ complex regex patterns extract useful text from dataset.</p> <h3 id="creating-vocabulary">Creating Vocabulary</h3> <p>Vocabulary refers to a set of unique words that occur in a given corpus or dataset. It represents the entire range of words used in the text data being analyzed. In python dialect, it is a dictionary for all possible words in corpus mapped to numerical IDs.</p> <p>To understand, imagine you have a large collection of books. Each book contains many different words, right? Now, if you were to make a list of all the unique words that appear across all the books, that list would be your vocabulary.</p> <p>So, essentially, a vocabulary is like a dictionary of words used in a specific context or dataset. It includes every distinct word found in the text data, without repetition.</p> <p>For example, if you were analyzing a set of news articles, your vocabulary might include words like ‚Äúpolitics,‚Äù ‚Äúelection,‚Äù ‚Äúeconomy,‚Äù and so on.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/vocab-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/vocab-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/vocab-1400.webp"/> <img src="/assets/img/vocab.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="vocabulary illustration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Illustration for creating tokenizer vocabulary.The values are taken from tiktoken encoding.</div> <p><strong>Qestion: any number and any range would do the job?</strong> Yes, the sole purpose of vocabulary is lookup, string to id and vice versa. In case of size, it is task dependent.</p> <p>Following code will demonstrate a simple implementation (reference to previous code example):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique_words</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">)))</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">unique_words</span><span class="p">)}</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span> <span class="nf">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

<span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">Himalayan</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">Nepal</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">breathtaking</span><span class="sh">'</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">explore</span><span class="sh">'</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">mountains</span><span class="sh">'</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="p">(</span><span class="sh">'</span><span class="s">traveled</span><span class="sh">'</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
</code></pre></div></div> <p>Using the prior knowledge, lets create a very simple tokenizer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ToeknizerV1</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">:</span><span class="nb">dict</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="o">=</span>  <span class="n">vocab</span>
        <span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="c1"># use regex to tokenize the input text
</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">([,.?_!</span><span class="sh">"</span><span class="s">()\']|--|\s)</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
        
        <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">idxs</span>
    
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\s+([,.?!</span><span class="sh">"</span><span class="s">()\'])</span><span class="sh">'</span><span class="p">,</span> <span class="sa">r</span><span class="sh">'</span><span class="s">\1</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">text</span>
</code></pre></div></div> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/enc-dec-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/enc-dec-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/enc-dec-1400.webp"/> <img src="/assets/img/enc-dec.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="vocabulary illustration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Illustration for encoding-decoding in tokenizer.</div> <p>Lets see it in action</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer_v1</span> <span class="o">=</span> <span class="nc">ToeknizerV1</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">idxs</span> <span class="o">=</span> <span class="n">tokenizer_v1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">tokenizer_va</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">idxs</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p>The encoder output is <code class="language-plaintext highlighter-rouge">[IDS]</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>The decoder outpus is <code class="language-plaintext highlighter-rouge">'str'</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">'</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">'</span>
</code></pre></div></div> <p>Try this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">try_input</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Kathmandu is capital city of Nepal</span><span class="sh">"</span>
<span class="n">tokenizer_v1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
</code></pre></div></div> <p><strong>oops‚Ä¶</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">KeyError</span><span class="p">:</span> <span class="sh">'</span><span class="s">Kathmandu</span><span class="sh">'</span>
</code></pre></div></div> <h4 id="special-tokens">Special tokens</h4> <p>The token <code class="language-plaintext highlighter-rouge">'Kathmandu'</code> does not exists in our vocab. Hence, the vocab should be rich enough to afford all tokens. However, a quick fix is to add special tokens such as <code class="language-plaintext highlighter-rouge">&lt;|unk|&gt;</code> , <code class="language-plaintext highlighter-rouge">&lt;|sos|&gt;</code>, and <code class="language-plaintext highlighter-rouge">&lt;|eos|&gt;</code>. Lets extend the unique words and vocabulary by adding special tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique_words</span><span class="p">.</span><span class="nf">extend</span><span class="p">([</span><span class="sh">"</span><span class="s">&lt;|unk|&gt;</span><span class="sh">"</span> <span class="p">,</span> <span class="sh">"</span><span class="s">&lt;|sos|&gt;</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">&lt;|eos|&gt;</span><span class="sh">"</span><span class="p">])</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">unique_words</span><span class="p">)}</span>
<span class="nf">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</code></pre></div></div> <p>New vocab list:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">Himalayan</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">Nepal</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">breathtaking</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">explore</span><span class="sh">'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">mountains</span><span class="sh">'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">traveled</span><span class="sh">'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">&lt;|unk|&gt;</span><span class="sh">'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">&lt;|sos|&gt;</span><span class="sh">'</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">&lt;|eos|&gt;</span><span class="sh">'</span><span class="p">:</span> <span class="mi">12</span><span class="p">}</span>
</code></pre></div></div> <p>wait wait‚Ä¶. we still need to modify <code class="language-plaintext highlighter-rouge">Tokenizer</code> encoding method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ToeknizerV2</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">:</span><span class="nb">dict</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="o">=</span>  <span class="n">vocab</span>
        <span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="c1"># use regex to tokenize the input text
</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">([,.?_!</span><span class="sh">"</span><span class="s">()\']|--|\s)</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">if</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span> <span class="k">else</span> <span class="sh">"</span><span class="s">&lt;|unk|&gt;</span><span class="sh">"</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        
        <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">s_to_i</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">idxs</span>
    
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">i_to_s</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\s+([,.?!</span><span class="sh">"</span><span class="s">()\'])</span><span class="sh">'</span><span class="p">,</span> <span class="sa">r</span><span class="sh">'</span><span class="s">\1</span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">text</span>
</code></pre></div></div> <p>Test out new tokenizer with <code class="language-plaintext highlighter-rouge">out-of-bag/vocabulary (OOB)</code> text.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">try_input</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Kathmandu is capital city of Nepal.&lt;|eos|&gt;</span><span class="sh">"</span>
<span class="n">t_v2</span> <span class="o">=</span> <span class="nc">ToeknizerV2</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">t_v2</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">try_input</span><span class="p">)</span>
</code></pre></div></div> <p>The encoder output</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t_v2</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">t_v2</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">try_input</span><span class="p">))</span>
</code></pre></div></div> <p>the decoder output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">'</span><span class="s">&lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; Nepal. &lt;|eos|&gt;</span><span class="sh">'</span>
</code></pre></div></div> <p>umm‚Ä¶.. we solve the error but it is not going be useful during model training. In the following section we will develop BPE to address such issue.</p> <hr/> <h3 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h3> <p>Byte Pair Encoding (BPE) is a subword tokenization technique used in natural language processing (NLP) to break down words into smaller, meaningful units called subword tokens. This technique is widely used in tasks such as machine translation, text generation, and language modeling.</p> <p>Similar tokenization techniques include WordPiece and SentencePiece, which are also subword tokenization methods that segment words into smaller units. These techniques are particularly useful for handling out-of-vocabulary words, morphologically rich languages, and reducing the size of the vocabulary in NLP models.</p> <p>Imagine you have a large collection of words in a language, like English. Some words are very common, like ‚Äúthe‚Äù or ‚Äúand,‚Äù while others are rare or even completely new. Byte Pair Encoding helps in representing all these words by breaking them down into smaller parts called subword tokens.</p> <h4 id="algorithm">Algorithm</h4> <ul> <li><strong>Step 1: Vocabulary initialization</strong> - Start with a vocabulary containing all unique characters present in the training data.</li> <li><strong>Step 2: Merging:</strong>- Iteratively merge the most frequent pair of adjacent subword tokens in the vocabulary. Repeat this process for a specified number of iterations or until a certain vocabulary size is reached.</li> <li><strong>Step 3: Vocabulary Expansion</strong> - After each merge, update the vocabulary to include the newly created subword tokens.</li> <li><strong>Step 4: Tokenization</strong> - Segment input text into subword tokens based on the learned vocabulary. Replace words not in the vocabulary with a special token, such as <code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code> for unknown.</li> </ul> <p>By iteratively applying these steps, Byte Pair Encoding effectively captures both frequent and rare patterns in the data, leading to compact and efficient representations for a wide range of words in the language.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bpe-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bpe-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bpe-1400.webp"/> <img src="/assets/img/bpe.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="vocabulary illustration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Illustration for byte pair encoding tokenizer. It breaks down unknown words into subwords and characters.</div> <p>Let‚Äôs use Byte Pair Encoding (BPE) to tokenize the input text. We‚Äôll start by initializing the vocabulary with individual unique characters, then iteratively merge the most frequent pairs of adjacent subword tokens. For this example, let‚Äôs perform two iterations of BPE:</p> <p><strong>Initialization:</strong></p> <ul> <li>Vocabulary: <code class="language-plaintext highlighter-rouge">{"I", " ", "t", "r", "a", "v", "e", "l", "d", "N", "p", "l", "o", "x", "u", "h", "b", "m", "i", "n", "s", "."}</code></li> </ul> <p><strong>Iteration 1:</strong></p> <ul> <li>Most frequent pair: <code class="language-plaintext highlighter-rouge">("t", "o")</code></li> <li>Merge <code class="language-plaintext highlighter-rouge">"t"</code> and <code class="language-plaintext highlighter-rouge">"o"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"to"</code>.</li> <li>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"I", " ", "to", "r", "a", "v", "e", "l", "d", "N", "p", "l", "o", "x", "u", "h", "b", "m", "i", "n", "s", "."}</code></li> </ul> <p><strong>Iteration 2:</strong></p> <ul> <li>Most frequent pair: <code class="language-plaintext highlighter-rouge">("e", " ")</code></li> <li>Merge <code class="language-plaintext highlighter-rouge">"e"</code> and <code class="language-plaintext highlighter-rouge">" "</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"e "</code>.</li> <li>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"I", " ", "to", "r", "a", "v", "e ", "l", "d", "N", "p", "l", "o", "x", "u", "h", "b", "m", "i", "n", "s", "."}</code></li> </ul> <p>Now, let‚Äôs tokenize the text using this vocabulary:</p> <p><strong>Text tokenization:</strong> <code class="language-plaintext highlighter-rouge">["I", " ", "trav", "eled", " ", "to", " ", "N", "ep", "al", " ", "to", " ", "exp", "lor", "e", " ", "the", " ", "br", "eat", "htaking", " ", "H", "ima", "lay", "an", " ", "m", "oun", "tains", "."]</code></p> <p>This tokenization captures both common patterns like <code class="language-plaintext highlighter-rouge">"to"</code> and <code class="language-plaintext highlighter-rouge">"e "</code> as well as less frequent patterns like ‚Äútrav‚Äù and ‚Äúmoun‚Äù, resulting in a more compact representation of the text compared to simple word tokenization.</p> <h4 id="bpe-implmentation">BPE Implmentation</h4> <p>We are going to use unicode characters to make more generic tokenizer. Lets create dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">regex</span> <span class="k">as</span> <span class="n">re</span>

<span class="n">raw_demo_text</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">üòÑ 0123456789 Webb telescope probably didn</span><span class="sh">'</span><span class="s">t find life on an exoplanet -- yet Claims of biosignature gas detection were premature. Recent reports of NASA</span><span class="sh">'</span><span class="s">s James Webb Space Telescope finding signs of life on a distant planet understandably sparked excitement. A new study challenges this finding, but also outlines how the telescope might verify the presence of the life-produced gas. Source:University of California - Riverside. ‡•®‡•¶ ‡§µ‡•à‡§∂‡§æ‡§ñ, ‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç ‡•§ ‡§µ‡•á‡§∏‡•ç‡§ü‡§á‡§®‡•ç‡§°‡§ø‡§ú ‚Äò‡§è‚Äô‡§≤‡•á ‡§ö‡•å‡§•‡•ã ‡§ü‡•Ä ‡•®‡•¶ ‡§ñ‡•á‡§≤‡§Æ‡§æ ‡§ò‡§∞‡•á‡§≤‡•Å ‡§ü‡•ã‡§≤‡•Ä ‡§®‡•á‡§™‡§æ‡§≤‡§≤‡§æ‡§à ‡•®‡•Æ ‡§∞‡§®‡§≤‡•á ‡§π‡§∞‡§æ‡§â‡§Å‡§¶‡•à ‡§è‡§ï ‡§ñ‡•á‡§≤ ‡§Ö‡§ó‡§æ‡§µ‡•à ‡§∏‡§ø‡§∞‡§ø‡§ú ‡§ú‡§ø‡§§‡•á‡§ï‡•ã ‡§õ ‡•§‡§™‡§æ‡§Å‡§ö ‡§ñ‡•á‡§≤‡§ï‡•ã ‡§∏‡§ø‡§∞‡§ø‡§ú‡§Æ‡§æ ‡§µ‡•á‡§∏‡•ç‡§ü‡§á‡§®‡•ç‡§°‡§ø‡§ú ‡•©‚Äì‡•ß ‡§≤‡•á ‡§Ö‡§ò‡§ø ‡§õ ‡•§ ‡§™‡§π‡§ø‡§≤‡•ã ‡§ñ‡•á‡§≤ ‡§π‡§æ‡§∞‡•á ‡§™‡§®‡§ø ‡§§‡•ç‡§Ø‡§∏‡§™‡§õ‡§ø ‡§≤‡§ó‡§æ‡§§‡§æ‡§∞ ‡§§‡•Ä‡§® ‡§ñ‡•á‡§≤‡§Æ‡§æ ‡§ú‡§ø‡§§‡§ï‡§æ ‡§∏‡§æ‡§• ‡§∏‡§ø‡§∞‡§ø‡§ú ‡§ú‡§ø‡§§‡•á‡§ï‡•ã ‡§π‡•ã ‡•§ ‡•ß‡•ß‡•¶ ‡§∞‡§®‡§ï‡•ã ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§™‡§õ‡•ç‡§Ø‡§æ‡§è‡§ï‡•ã ‡§®‡•á‡§™‡§æ‡§≤ ‡•®‡•¶ ‡§ì‡§≠‡§∞‡§Æ‡§æ ‡•ß‡•Æ‡•ß ‡§∞‡§®‡§Æ‡§æ ‡§Ö‡§≤ ‡§Ü‡§â‡§ü ‡§≠‡§è‡§ï‡•ã ‡§õ ‡•§ ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡§æ ‡§ï‡§™‡•ç‡§§‡§æ‡§® ‡§∞‡•ã‡§π‡§ø‡§§ ‡§™‡•å‡§°‡•á‡§≤‡§≤‡•á ‡•™‡•≠ ‡§¨‡§≤‡§Æ‡§æ ‡•≠ ‡§ö‡•å‡§ï‡§æ ‡§∞ ‡•´ ‡§õ‡§ï‡•ç‡§ï‡§æ‡§∏‡§π‡§ø‡§§ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§ß‡§ø‡§ï ‡•Æ‡•® ‡§∞‡§® ‡§¨‡§®‡§æ‡§è ‡§™‡§®‡§ø ‡§ú‡§ø‡§§‡•ç‡§®‡§≤‡§æ‡§à ‡§™‡§∞‡•ç‡§Ø‡§æ‡§™‡•ç‡§§ ‡§≠‡§è‡§® ‡•§ ‡§∞‡•ã‡§π‡§ø‡§§‡§≤‡•á ‡§™‡§π‡§ø‡§≤‡•ã ‡§ñ‡•á‡§≤‡§Æ‡§æ ‡•ß‡•ß‡•®, ‡§¶‡•ã‡§∏‡•ç‡§∞‡•ã ‡§ñ‡•á‡§≤‡§Æ‡§æ ‡•≠‡•ß ‡§∞‡§® ‡§∞ ‡§§‡•á‡§∏‡•ç‡§∞‡•ã ‡§ñ‡•á‡§≤‡§Æ‡§æ ‡§µ‡§ø‡§∂‡•ç‡§∞‡§æ‡§Æ ‡§ó‡§∞‡•á‡§ï‡§æ ‡§•‡§ø‡§è ‡•§ ‡§∞‡•ã‡§π‡§ø‡§§ ‡§¨‡§æ‡§π‡•á‡§ï ‡§Ü‡§ú‡§ï‡•ã ‡§ñ‡•á‡§≤‡§Æ‡§æ ‡§™‡§®‡§ø ‡§Ö‡§®‡•ç‡§Ø ‡§¨‡•ç‡§Ø‡§æ‡§ü‡§∞‡§≤‡•á ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•á‡§®‡§®‡•ç ‡•§ ‡§∏‡•ç‡§™‡§ø‡§®‡§∞ ‡§π‡•á‡§°‡§® ‡§µ‡§æ‡§≤‡•ç‡§∏ ‡§ú‡•Å‡§®‡§ø‡§Ø‡§∞‡§ï‡•ã ‡•ß‡•™‡§î‡§Ç ‡§ì‡§≠‡§∞‡§ï‡•ã ‡§§‡•á‡§∏‡•ç‡§∞‡•ã ‡§¨‡§≤‡§Æ‡§æ ‡§∞‡•ã‡§π‡§ø‡§§ ‡§≤‡§ô‡§Ö‡§´‡§Æ‡§æ ‡§Æ‡•ç‡§Ø‡§æ‡§•‡•ç‡§Ø‡•Å ‡§´‡§∞‡•ç‡§°‡§¨‡§æ‡§ü ‡§ï‡•ç‡§Ø‡§æ‡§ö ‡§Ü‡§â‡§ü ‡§≠‡§è‡§™‡§õ‡§ø ‡§∂‡§§‡§ï‡§ï‡•ã ‡§Ö‡§µ‡§∏‡§∞ ‡§ó‡•Å‡§Æ‡§æ‡§è‡§ï‡§æ ‡§•‡§ø‡§è ‡•§ ‡§ì‡§™‡§®‡§∞ ‡§Ü‡§∏‡§ø‡§´ ‡§∂‡•á‡§ñ ‡§∂‡•Ç‡§®‡•ç‡§Ø ‡§∞ ‡§ï‡•Å‡§∂‡§≤ ‡§≠‡•Å‡§∞‡•ç‡§§‡•á‡§≤ ‡•ß ‡§∞‡§®‡§Æ‡§æ ‡§Ü‡§â‡§ü ‡§≠‡§è‡§ï‡§æ ‡§•‡§ø‡§è ‡•§ ‡§ï‡•Å‡§∂‡§≤ ‡§Æ‡§≤‡•ç‡§≤ ‡•™, ‡§∏‡§®‡•ç‡§¶‡•Ä‡§™ ‡§ú‡•ã‡§∞‡§æ, ‡§¶‡•Ä‡§™‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§∏‡§ø‡§Ç‡§π ‡§ê‡§∞‡•Ä ‡§∞ ‡§ó‡•Å‡§≤‡§∂‡§® ‡§ù‡§æ ‡•ß‡•Ø‚Äì‡•ß‡•Ø ‡§∞‡§®‡§Æ‡§æ ‡§Ü‡§â‡§ü ‡§≠‡§è ‡•§ ‡§Ö‡§≠‡§ø‡§®‡§æ‡§∂ ‡§¨‡•ã‡§π‡§∞‡§æ ‡•Æ ‡§¨‡§≤‡§Æ‡§æ ‡•® ‡§ö‡•å‡§ï‡§æ ‡§∞ ‡•ß ‡§õ‡§ï‡•ç‡§ï‡§æ‡§∏‡§π‡§ø‡§§ ‡•ß‡•≠ ‡§∞ ‡§∏‡•ã‡§Æ‡§™‡§æ‡§≤ ‡§ï‡§æ‡§Æ‡•Ä ‡•ß‡•¶ ‡§∞‡§®‡§Æ‡§æ ‡§Ü‡§â‡§ü ‡§≠‡§è ‡•§ ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™!</span><span class="sh">'''</span>

</code></pre></div></div> <p>In this implementation, we will be using <code class="language-plaintext highlighter-rouge">utf-8</code> encode for some advatanges. lets map all <code class="language-plaintext highlighter-rouge">str</code> to <code class="language-plaintext highlighter-rouge">UTF-8</code> format and then convert it into a list of integers representing the bytes of the UTF-8 encoded string.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">raw_demo_text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">====================================</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Text length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">raw_demo_text</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">raw_demo_text</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">====================================</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="mi">50</span><span class="p">])</span>

</code></pre></div></div> <p>Output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">====================================</span>
<span class="n">Text</span> <span class="n">length</span><span class="p">:</span> <span class="mi">1349</span>

<span class="err">üòÑ</span> <span class="mi">0123456789</span> <span class="n">Webb</span> <span class="n">telescope</span> <span class="n">probably</span> <span class="n">didn</span><span class="sh">'</span><span class="s">t find life on an exoplanet -- yet Claims of biosignature 
====================================
Token length: 2846
[240, 159, 152, 132, 32, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 32, 87, 101, 98, 98, 32, 116, 101, 108, 101, 115, 99, 111, 112, 101, 32, 112, 114, 111, 98, 97, 98, 108, 121, 32, 100, 105, 100, 110, 39, 116, 32, 102, 105, 110]

</span></code></pre></div></div> <p>Lets create a function that creates a consecutive pairs of chracters present in the dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    @params: list[int]
    returns: {(pairs: tuples): int(occurance) }
    </span><span class="sh">'''</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">counts</span>
</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">)</span>
</code></pre></div></div> <p>output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{(</span><span class="mi">240</span><span class="p">,</span> <span class="mi">159</span><span class="p">):</span> <span class="mi">15</span><span class="p">,</span> <span class="p">(</span><span class="mi">159</span><span class="p">,</span> <span class="mi">152</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">152</span><span class="p">,</span> <span class="mi">132</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">132</span><span class="p">,</span> <span class="mi">32</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">48</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">49</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">49</span><span class="p">,</span> <span class="mi">50</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">52</span><span class="p">,</span> <span class="mi">53</span><span class="p">):</span> <span class="mi">1</span><span class="p">}</span>
</code></pre></div></div> <p>lets sort them by value (occurance) and pick top k.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(((</span><span class="n">v</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">pair_stats</span><span class="p">.</span><span class="nf">items</span><span class="p">()),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="p">[(</span><span class="mi">531</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">164</span><span class="p">)),</span> <span class="p">(</span><span class="mi">177</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">165</span><span class="p">)),</span> <span class="p">(</span><span class="mi">171</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span> <span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="mi">190</span><span class="p">)),</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="mi">176</span><span class="p">)),</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="p">(</span><span class="mi">176</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="mi">178</span><span class="p">))]</span>
</code></pre></div></div> <p>Lets understand the token merge and vocabulary extension in BPE. For instance,</p> <p>vacob: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d"}</code></p> <p>input_text (dataset): <code class="language-plaintext highlighter-rouge">"aaabdaaabac"</code></p> <p>pair tokens: <code class="language-plaintext highlighter-rouge">{"aa": 3, "ab": 1, "bd": 1, "da": 2, "ac": 1}</code></p> <p>Iteration 1:</p> <p>Most frequent pair: ‚Äúaa‚Äù (frequency = 3)</p> <p>Merge <code class="language-plaintext highlighter-rouge">"a"</code> and <code class="language-plaintext highlighter-rouge">"a"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"Z"</code>.</p> <p>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d", "Z"}</code>, and updated text <code class="language-plaintext highlighter-rouge">ZabdZabac</code></p> <p>new pair tokens: <code class="language-plaintext highlighter-rouge">{"ab":2, "bd":1,"dZ":1, "ac":1 }</code></p> <p>Iteration 2:</p> <p>Most frequent pair: ‚Äúab‚Äù (frequency = 2)</p> <p>Merge <code class="language-plaintext highlighter-rouge">"a"</code> and <code class="language-plaintext highlighter-rouge">"b"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"Y"</code>.</p> <p>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d", "Z", "Y"}</code>, and updated text <code class="language-plaintext highlighter-rouge">ZYdZYac</code></p> <p>new pair tokens: <code class="language-plaintext highlighter-rouge">{"ZY":2, "Yd":1, "ac":1}</code></p> <p>Iteration 3:</p> <p>Most frequent pair: <code class="language-plaintext highlighter-rouge">"ZY"</code> (frequency = 2)</p> <p>Merge <code class="language-plaintext highlighter-rouge">"Z"</code> and <code class="language-plaintext highlighter-rouge">"Y"</code> to create a new subword token <code class="language-plaintext highlighter-rouge">"X"</code>.</p> <p>Updated vocabulary: <code class="language-plaintext highlighter-rouge">{"a", "b", "c", "d", "Z", "Y", "X"}</code>, and updated text <code class="language-plaintext highlighter-rouge">XdXac</code></p> <p><strong>Here we have compressed the sequence length from 11 to 5, but increased the vocab size from 4 to 7.</strong></p> <p>Lets see this in code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="c1"># list of ints (ids), replace all consecutive occurances of pair with new token idx 
</span>    <span class="n">newids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span><span class="nf">len </span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
        <span class="c1"># if we are not at very last position AND the pair matches, replace it 
</span>        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">newids</span>

</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">demo: </span><span class="si">{</span><span class="nf">merge</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="mi">999</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>output: <code class="language-plaintext highlighter-rouge">demo: [1, 2, 999, 5]</code> pair <code class="language-plaintext highlighter-rouge">(2,3)</code> is replaced with <code class="language-plaintext highlighter-rouge">999</code>.</p> <p>Here, we create the vocab size of 333 (a hyperparameter), and merge tokens. Here, we are using utf-8 that is 256 tokens of raw bytes. The merged token will have IDs from 256. Following codes runs 77 (333-256) iterations and creates new merged tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">voacab_size</span> <span class="o">=</span> <span class="mi">333</span>
<span class="n">required_merges</span> <span class="o">=</span> <span class="n">voacab_size</span> <span class="o">-</span> <span class="mi">256</span>
<span class="c1"># new copy of ids 
</span><span class="n">ids</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">required_merges</span><span class="p">):</span>
    <span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pair_stats</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">+</span> <span class="n">i</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">merging </span><span class="si">{</span><span class="n">pair</span><span class="si">}</span><span class="s"> into new token </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="nf">merge</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>merging (224, 164) into new token 256
merging (224, 165) into new token 257
merging (32, 256) into new token 258
merging (256, 190) into new token 259
merging (257, 141) into new token 260
merging (260, 256) into new token 261
merging (256, 191) into new token 262
.....................................
.....................................
merging (116, 101) into new token 332
</code></pre></div></div> <p>Compression result:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">toekn length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="s">, ids length: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s">compression ratio: </span><span class="si">{</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">))</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">X</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">toekn length: 2846, ids length: 1094 compression ratio: 2.60X</code>. We have compressed the text by 2.6 X.</p> <p>create the vocab. it stores the byte information for given IDs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span> <span class="n">idx</span><span class="p">:</span> <span class="nf">bytes</span><span class="p">([</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span> <span class="p">}</span>
<span class="nf">for </span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">),</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">p0</span><span class="p">]</span> <span class="o">+</span> <span class="n">vocab</span><span class="p">[</span><span class="n">p1</span><span class="p">]</span>
</code></pre></div></div> <p>Here, we will create a encoder and decoder block. The encoder returns the token IDs <code class="language-plaintext highlighter-rouge">list[int]</code> for a given text, and decoder returns the text for given <code class="language-plaintext highlighter-rouge">list[int]</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># given a text, return token (list of integers)
</span><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="c1"># extract raw bytes, converted to list of integers
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">))</span>
    <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">pair</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">merges</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">pair</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">merges</span><span class="p">:</span> <span class="k">break</span> <span class="c1"># nothing can be merged
</span>        <span class="n">idx</span> <span class="o">=</span> <span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="nf">merge</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokens</span>    

<span class="c1"># given list of integers, returns python string
</span><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="sa">b</span><span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="sh">"</span><span class="s">replace</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span>
    
</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">enc</span> <span class="o">=</span> <span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">"</span><span class="p">)</span>
<span class="n">dec</span> <span class="o">=</span> <span class="nf">decode</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">encoded: </span><span class="si">{</span><span class="n">enc</span><span class="si">}</span><span class="se">\n</span><span class="s">decoded:</span><span class="si">{</span><span class="n">dec</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">encoded: [73, 32, 116, 114, 97, 118, 101, 108, 101, 100, 32, 116, 111, 32, 78, 101, 112, 97, 108, 32, 116, 111, 32, 101, 120, 112, 108, 111, 114, 277, 116, 104, 277, 98, 114, 101, 97, 116, 104, 116, 97, 107, 304, 103, 32, 72, 105, 109, 97, 108, 97, 121, 317, 32, 109, 111, 117, 110, 116, 97, 304, 115, 46]</code></p> <p><code class="language-plaintext highlighter-rouge">decoded:I traveled to Nepal to explore the breathtaking Himalayan mountains.</code></p> <p>Now, lets create simple BPE version 1 tokenizer. This tokenizer will first take dataset i.e. text to construct vocabulary and merges with highly frequent pairs until desired vocab size. The instance then can encode and decode the test and IDs repectively.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BPEV1</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span><span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__parse_text</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">__create_vocab</span><span class="p">()</span>
        <span class="k">del</span> <span class="n">self</span><span class="p">.</span><span class="n">tokens</span>

    
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__parse_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">pair_stats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__get_pair_data</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">pair</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span> <span class="k">break</span> <span class="c1"># nothing can be merged
</span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__merge</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tokens</span>  
        
        
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="sa">b</span><span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">)</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="sh">"</span><span class="s">replace</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">text</span>
    
    
    <span class="k">def</span> <span class="nf">__parse_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">__get_pair_data</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
        <span class="sh">'''</span><span class="s">
        @params: list[int]
        returns: {(pairs: tuples): int(occurance) }
        </span><span class="sh">'''</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">counts</span>

    <span class="k">def</span> <span class="nf">__merge</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">pair</span><span class="p">:</span><span class="nb">tuple</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
        <span class="n">newids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span><span class="nf">len </span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
            <span class="c1"># if we are not at very last position AND the pair matches, replace it 
</span>            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">newids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">newids</span>
    <span class="k">def</span> <span class="nf">__create_vocab</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">new_vocabs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">256</span>
        <span class="k">assert</span> <span class="n">new_vocabs</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">vocab size must be greater than 256</span><span class="sh">"</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span> <span class="n">idx</span><span class="p">:</span> <span class="nf">bytes</span><span class="p">([</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)}</span>
        <span class="c1"># merging
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">new_vocabs</span><span class="p">):</span>
            <span class="n">pair_stats</span> <span class="o">=</span> <span class="nf">get_pair_data</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">)</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pair_stats</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">+</span> <span class="n">i</span>
            <span class="n">self</span><span class="p">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__merge</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        
        <span class="nf">for </span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">),</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">p0</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">p1</span><span class="p">]</span>
    
</code></pre></div></div> <p>example usage:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">bpev1</span> <span class="o">=</span> <span class="nc">BPEV1</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">raw_demo_text</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">333</span><span class="p">)</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">I traveled to Nepal to explore the breathtaking Himalayan mountains.</span><span class="sh">"</span><span class="p">)</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">encoded: </span><span class="si">{</span><span class="n">enc</span><span class="si">}</span><span class="se">\n</span><span class="s">decoded:</span><span class="si">{</span><span class="n">dec</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p>Output: look at previous section.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># validation
</span><span class="n">test_txt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s"> ‡§Æ ‡§∞‡•ã‡§Ø‡§≤ ‡§®‡•á‡§™‡§æ‡§≤ ‡§ó‡§≤‡•ç‡§´ ‡§ï‡•ç‡§≤‡§¨‡§Æ‡§æ ‡§ñ‡•á‡§≤‡•ç‡§•‡•á‡§Ç ‡•§ ‡§â‡§π‡§æ‡§Å‡§≤‡•á ‡§∏‡•ã‡§π‡•Ä ‡§∏‡§Æ‡§Ø ‡§ó‡§≤‡•ç‡§´ ‡§ñ‡•á‡§≤‡•ç‡§® ‡§∏‡•Å‡§∞‡•Å ‡§ó‡§∞‡•ç‡§®‡•Å ‡§≠‡§è‡§ï‡•ã ‡§π‡•ã ‡•§ ‡§â‡§π‡§æ‡§Å‡§≤‡•á ‡§Æ‡§≤‡§æ‡§à ‡§∏‡§æ‡§®‡•ã ‡§≠‡§æ‡§á‡§ï‡•ã ‡§∞‡•Å‡§™‡§Æ‡§æ ‡§Æ‡§æ‡§Ø‡§æ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§•‡•ç‡§Ø‡•ã ‡•§</span><span class="sh">'''</span>
<span class="n">val_txt</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">bpev1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">txt</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">test_txt</span> <span class="o">==</span> <span class="n">val_txt</span><span class="p">)</span>
</code></pre></div></div> <p>output: <code class="language-plaintext highlighter-rouge">True</code></p> <p>Finally, we have implemented the Byte pair encoder and decoder on small text dataset. However, this can be extended to larger dataset as well. It is robust and capable to accomodate most of the unseen texts.</p> <p>Take a look at the following, example python script:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
import tiktoken
sample_text = </span><span class="sh">"</span><span class="s">‡§Ü‡§á‡§è‡§Æ‡§á ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑ ‡§ö‡§®‡•ç‡§¶‡•ç‡§∞ ‡§™‡•ç‡§∞‡§∏‡§æ‡§¶ ‡§¢‡§ï‡§æ‡§≤‡§≤‡•á ‡§¨‡§æ‡§π‡•ç‡§∞‡§ñ‡§∞‡•Ä‡§∏‡§Å‡§ó ‡§ó‡§∞‡•á‡§ï‡•ã ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§</span><span class="sh">"</span><span class="s">
#GPT2 
enc = tiktoken.get_encoding(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="s">)
print(enc.encode(sample_text))

#GPT4 (merge space)
enc = tiktoken.get_encoding(</span><span class="sh">"</span><span class="s">cl100k_base</span><span class="sh">"</span><span class="s">)
gpt4tokens = enc.encode(sample_text)
print(gpt4tokens)</span><span class="sh">'''</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">bpev1</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">tt</span><span class="p">)</span>
<span class="n">bpev1</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>

</code></pre></div></div> <p>Output: <code class="language-plaintext highlighter-rouge">'\nimport tiktoken\nsample_text = "‡§Ü‡§á‡§è‡§Æ‡§á ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑ ‡§ö‡§®‡•ç‡§¶‡•ç‡§∞ ‡§™‡•ç‡§∞‡§∏‡§æ‡§¶ ‡§¢‡§ï‡§æ‡§≤‡§≤‡•á ‡§¨‡§æ‡§π‡•ç‡§∞‡§ñ‡§∞‡•Ä‡§∏‡§Å‡§ó ‡§ó‡§∞‡•á‡§ï‡•ã ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§"\n#GPT2 \nenc = tiktoken.get_encoding("gpt2")\nprint(enc.encode(sample_text))\n\n#GPT4 (merge space)\nenc = tiktoken.get_encoding("cl100k_base")\ngpt4tokens = enc.encode(sample_text)\nprint(gpt4tokens)'</code></p> <p>Aweesome !!</p> <h2 id="conclusion">Conclusion</h2> <p>We have discussed embedding and tkonization techniques primarily what, why, and how? Every NLP tasks need to be tokenized and embedded in order to train, fine-tune, and generate text from deep learning models such as BERT, GPT, llama, mistral and etc. Embedding is extended to encode any non numerical data such as signals and vision.</p> <hr/> <h2 id="citations">Citations</h2> <ul> <li>Thanks to GenAI for content writing.</li> <li>A <a href="https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=6359s">tutorial</a> by A. Karpathy.</li> </ul>]]></content><author><name>Tsuyog Basnet</name></author><category term="Embedding"/><summary type="html"><![CDATA[Understanding embedding and toeknization in natural language processing. An implementation of Byte Pair Encoding.]]></summary></entry><entry><title type="html">RAG - Retrieval Augmented Generation</title><link href="https://pvsnp9.github.io/blog/2024/rag/" rel="alternate" type="text/html" title="RAG - Retrieval Augmented Generation"/><published>2024-04-12T00:00:00+00:00</published><updated>2024-04-12T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/rag</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/rag/"><![CDATA[<h2 id="introduction">Introduction</h2> <blockquote> <p><strong>AI: I am sorry, i can not provide you the answer without context or I was trained on before date.</strong></p> </blockquote> <p>Retrieval Augmented Generation (RAG) combines retrieval-based and generative models in natural language processing to produce contextually relevant and coherent responses by first retrieving relevant passages or documents and then using them to guide the generative model.</p> <p>At its core, Retrieval Augmented Generation is a fusion of two fundamental approaches in NLP:</p> <ol> <li> <p><strong>Retrieval-Based Models</strong>: These models excel at retrieving relevant information from large corpora of text based on a given query or context. They leverage techniques like similarity search, or advanced methods such as dense retrieval with neural networks to efficiently fetch passages that are most likely to contain the desired information. In another words, finding most relevant documents for a input query based on specified algorithm (cosine similarity, or any distnace metrics).</p> </li> <li> <p><strong>Generative Models</strong>: On the other hand, generative models, particularly those based on transformers like GPT (Generative Pre-trained Transformer), have demonstrated remarkable proficiency in generating human-like text. They operate by predicting the next word or token in a sequence based on the preceding context, often trained on vast amounts of text data.</p> </li> </ol> <p>Retrieval Augmented Generation seeks to harness the strengths of both these approaches by integrating retrieval-based systems into the generative pipeline. The basic idea is to first retrieve a set of relevant passages or documents from a knowledge source (such as a large text corpus or a knowledge graph) and then use these retrieved contexts to guide the generative model in producing a more informed and contextually appropriate response.</p> <p>Here‚Äôs how the process typically unfolds:</p> <ol> <li> <p><strong>Retrieval</strong>: Given an input query or context, a retrieval-based model is employed to fetch a set of relevant documents or passages from a knowledge base. This retrieval step is crucial for providing the generative model with a rich source of contextual information.</p> </li> <li> <p><strong>Generation</strong>: The retrieved passages serve as the context for the generative model, which then generates a response based not only on the original input but also on the retrieved knowledge. By incorporating this additional context, the generative model can produce responses that are more coherent, informative, and contextually relevant.</p> </li> <li> <p><strong>Ranking</strong>: In some implementations, a ranking mechanism may be employed to select the most suitable response among multiple candidates generated by the generative model. This step ensures that the final output is of the highest quality and relevance.</p> </li> </ol> <p>The beauty of Retrieval Augmented Generation lies in its ability to combine the depth of knowledge retrieval with the creativity of generative models, resulting in responses that are not only fluent and coherent but also grounded in factual accuracy and contextual understanding. This approach has numerous applications across various domains, including question answering, conversational agents, content generation, and more.</p> <hr/> <h2 id="large-language-models">Large Language Models</h2> <p>Large Language Models (LLMs) stand as marvels of mathematical ingenuity intertwined with cutting-edge technology. At their core, LLMs rely on intricate neural network designs, often built upon the transformative Transformer architecture. These models boast layers of attention mechanisms, allowing them to adeptly capture intricate linguistic nuances and dependencies within text. One key mathematical concept underpinning LLMs is the notion of attention mechanisms, particularly prevalent in architectures like the Transformer model. Attention mechanisms enable the model to weigh the importance of different words or tokens in a sequence, allowing it to focus on relevant information while filtering out noise. This process involves matrix operations and vector manipulations, where attention scores are computed through dot products and softmax functions, creating a weighted representation of the input.</p> <p>Additionally, LLMs leverage sophisticated optimization algorithms, such as stochastic gradient descent and its variants, to iteratively adjust model parameters during training, minimizing a loss function that quantifies the disparity between predicted and actual text. Through these mathematical intricacies, LLMs harness the power of data and computation to transcend the boundaries of language understanding and generation.</p> <p>For instance, <code class="language-plaintext highlighter-rouge">P ["Nepal" | "Kathmandu is a city in ____"]</code>. In this example, <code class="language-plaintext highlighter-rouge">pormpt : "Kathmandu is a city in"</code> and prediction of LLM model <code class="language-plaintext highlighter-rouge">token: "Nepal"</code>. The LLM model simply spits out the probability of each word in predefined vocabulary. It accquires the knowledge such as context, relationship via training.</p> <p><strong>List of some LLMs</strong>:</p> <ol> <li><strong>GPT-3.5</strong> and <strong>GPT-4</strong> by <strong>OpenAI</strong>: These models power applications like <strong>ChatGPT</strong> and <strong>Microsoft Copilot</strong>¬≤.</li> <li><strong>PaLM</strong> by <strong>Google</strong>: A commercial LLM.</li> <li><strong>Gemini</strong> by Google: Currently used in the chatbot of the same name.</li> <li><strong>Grok</strong> by <strong>xAI</strong>: An intriguing LLM.</li> <li><strong>LLaMA</strong> family of open-source models by <strong>Meta</strong>.</li> <li><strong>Claude</strong> models by <strong>Anthropic</strong>.</li> <li><strong>Mistral AI</strong>‚Äôs open-source models.</li> <li><strong>DBRX</strong> by <strong>Databricks</strong>: An open-source LLM¬≤.</li> </ol> <h3 id="fine-tuning">Fine Tuning</h3> <p>Fine-tuning is the process of taking a pre-trained model and further training it on a specific task or dataset to enhance its performance for that particular objective. Essentially, fine-tuning enables the adjustment of the parameters of the pre-trained model to adapt it to the intricacies of the target task or domain.</p> <p>Fine-tuning techniques:</p> <ol> <li> <p><strong>LoRA (Low Rank Adaption of LLM)</strong>: LoRA is a fine-tuning technique designed specifically for Large Language Models (LLMs), such as GPT models. It focuses on efficiently adapting pre-trained LLMs to new tasks while mitigating the risk of catastrophic forgetting, which occurs when the model forgets previously learned knowledge while learning new information. LoRA achieves this by introducing low-rank adaptations to the model‚Äôs parameters during fine-tuning. By adjusting the rank of parameter matrices in the LLM, LoRA allows for more efficient adaptation to new tasks without significantly increasing computational overhead.</p> </li> <li> <p><strong>QLoRA (Quantized Low Rank Adaption of LLM)</strong>: QLoRA builds upon the principles of LoRA while introducing quantization to further optimize the fine-tuning process. In QLoRA, the low-rank adaptations of LLM parameters are quantized into a discrete set of values. This quantization serves to stabilize training and reduce memory requirements, making fine-tuning more computationally efficient. By combining low-rank adaptation with quantization, QLoRA enables effective fine-tuning of LLMs on diverse tasks with minimal computational overhead.</p> </li> </ol> <p>These techniques represent innovative approaches to fine-tuning LLMs for specific tasks, offering efficient solutions to adapt pre-trained models to new domains while preserving previously learned knowledge.Techniques like LoRA and QLoRA can unlock the full potential of LLMs across a wide range of natural language processing tasks and applications.</p> <p><strong>Fine-tuning and Pretraining a LLM is prohbitively expensive.</strong></p> <hr/> <h2 id="prompt-engineering">Prompt Engineering</h2> <p>Prompt engineering involves crafting tailored instructions or queries to guide AI models towards accurate and contextually relevant outputs. For instance, in a conversational AI system designed to provide movie recommendations based on user preferences, a good prompt might include specific criteria such as genre, release year, and preferred actors, such as ‚ÄúRecommend a Nepali movie released in the past five years starring either Bipin Karki or Dayahang Rai.‚Äù This nuanced instruction provides the AI model with clear guidance on the user‚Äôs preferences, enabling it to generate highly relevant and personalized recommendations, showcasing the intricate nature of prompt engineering in AI applications.</p> <p>The general format:</p> <ul> <li><code class="language-plaintext highlighter-rouge">Instructions</code>:<code class="language-plaintext highlighter-rouge"> You are an assistant .................</code></li> <li><code class="language-plaintext highlighter-rouge">Context</code>: <code class="language-plaintext highlighter-rouge">This is context ......</code></li> <li><code class="language-plaintext highlighter-rouge">Question/Query</code>: <code class="language-plaintext highlighter-rouge">Your question .....</code></li> <li><code class="language-plaintext highlighter-rouge">Answer</code>: <code class="language-plaintext highlighter-rouge">LLM generated response</code></li> </ul> <hr/> <h2 id="rag-pipeline">RAG Pipeline</h2> <p>RAG Pipeline consists of many components.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rag-pipeline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rag-pipeline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rag-pipeline-1400.webp"/> <img src="/assets/img/rag-pipeline.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <hr/> <h2 id="embedding-vectors">Embedding Vectors</h2> <p>In natural language processing (NLP), embedding refers to the process of representing words or tokens as numerical vectors in a continuous vector space. These vectors capture semantic relationships between words, enabling NLP models to understand and process textual data more effectively. BERT (Bidirectional Encoder Representations from Transformers) is a powerful model that generates word embeddings with rich contextual information.</p> <p>Let‚Äôs illustrate embedding with the example text <code class="language-plaintext highlighter-rouge">"Kathmandu is city of temple"</code> using BERT:</p> <p>In BERT, each word in the sentence is tokenized and represented as a vector. For instance, the word <code class="language-plaintext highlighter-rouge">"Kathmandu"</code> is tokenized into individual subwords or tokens, such as <code class="language-plaintext highlighter-rouge">["Kath", "##man", "##du"]</code>. Each token is embedded into a high-dimensional vector space.</p> <p>In the case of ‚ÄúKathmandu is city of temple,‚Äù BERT captures the contextual information of each token by considering its surrounding tokens. So, the embedding for <code class="language-plaintext highlighter-rouge">"Kath"</code> might be influenced by the tokens <code class="language-plaintext highlighter-rouge">"is"</code> and <code class="language-plaintext highlighter-rouge">"city,"</code> indicating its context within the sentence.</p> <p>Similarly, the embedding for <code class="language-plaintext highlighter-rouge">"city"</code> would capture its relationship with <code class="language-plaintext highlighter-rouge">"Kathmandu"</code> and <code class="language-plaintext highlighter-rouge">"temple"</code> in the sentence, enabling BERT to understand that <code class="language-plaintext highlighter-rouge">"city"</code> is associated with both <code class="language-plaintext highlighter-rouge">"Kathmandu"</code> and <code class="language-plaintext highlighter-rouge">"temple"</code> in this particular context.</p> <p>By generating embeddings that encode contextual information, BERT enables NLP models to grasp the nuanced meanings of words in different contexts. These embeddings serve as input to downstream tasks, allowing the model to make accurate predictions or generate relevant outputs based on the semantic information captured in the embeddings.</p> <h3 id="sentence-embedding">Sentence Embedding</h3> <p>Sentence embedding refers to the process of representing entire sentences or phrases as numerical vectors in a continuous vector space, capturing their semantic meaning and context. Let‚Äôs explore sentence embedding using the example ‚ÄúKathmandu is city of temple.‚Äù</p> <p>In sentence embedding, the sentence ‚ÄúKathmandu is city of temple‚Äù would be tokenized into individual words or subwords and processed to generate a single vector representation that encapsulates its semantic meaning and context.</p> <p>Using a pre-trained language model like BERT or Universal Sentence Encoder, each word or subword in the sentence is converted into a vector representation, and these individual embeddings are combined or aggregated to produce a final vector representation for the entire sentence.</p> <p>For example, the sentence ‚ÄúKathmandu is city of temple‚Äù might be tokenized into the following subwords: <code class="language-plaintext highlighter-rouge">["Kath", "##mandu", "is", "city", "of", "temple"]</code></p> <p>Each of these subwords is then embedded into a high-dimensional vector space. The embedding for the entire sentence is computed by aggregating or combining these individual subword embeddings, typically using techniques like averaging or pooling.</p> <p>The resulting sentence embedding captures the semantic meaning and context of the entire sentence ‚ÄúKathmandu is city of temple‚Äù in a continuous vector representation. This embedding can then be used as input to downstream NLP tasks such as sentiment analysis, text classification, or semantic similarity comparison between sentences, allowing NLP models to make accurate predictions or generate relevant outputs based on the semantic information encoded in the embedding.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sentence_emb-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sentence_emb-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sentence_emb-1400.webp"/> <img src="/assets/img/sentence_emb.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>How does the model know if two sentence have similar meaning ?</strong></p> <p>One of the popular method to find relationship between sentences (vectors in nutshell) is cosine similarity score. It measures the the angle between two vectors. A small angle results in high score that is also a highly similar vector.</p> <p>Given two n-dimensional vectors of attributes, $A$ and $B$, the cosine similarity, $cos(Œ∏)$, is represented using a dot product and magnitude as</p> \[S_C(A,B)\] \[cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A^2_n} \cdot \sqrt{\sum_{i=1}^n B^2_n}}\] <p>Yet, how to teach BERT to use desired similarity metrics like cosine and assure two sentences produce the similar result?</p> <hr/> <h2 id="sentence-bert">Sentence BERT</h2> <p>Sentence BERT (SBERT) is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model specifically designed for generating high-quality sentence embeddings. Unlike traditional BERT, which operates at the word level, SBERT processes entire sentences or phrases to generate embeddings that capture the semantic meaning and context of the input text.</p> <p>SBERT achieves this by fine-tuning the BERT architecture on a variety of sentence-level tasks, such as sentence similarity, paraphrase identification, and natural language inference. During training, SBERT learns to encode the semantic similarity between pairs of sentences, allowing it to generate embeddings that effectively capture the meaning of entire sentences.</p> <p>One key innovation of SBERT is the use of siamese or triplet network architectures, where multiple copies of the BERT model share weights and are trained to optimize a similarity metric between sentence pairs. This encourages SBERT to learn representations that are invariant to certain transformations (e.g., word order or paraphrasing) while emphasizing differences between dissimilar sentences.</p> <p>SBERT embeddings have demonstrated superior performance in various NLP tasks requiring sentence-level understanding, such as semantic textual similarity, text classification, and information retrieval.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sbert-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sbert-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sbert-1400.webp"/> <img src="/assets/img/sbert.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <hr/> <h2 id="vector-database">Vector Database</h2> <p>A vector database is a type of database that stores data in the form of vectors, typically numerical representations of objects or documents in a high-dimensional (predefined) vector space. These databases are designed to efficiently store and retrieve vector data, enabling various applications such as similarity search, recommendation systems, and information retrieval.</p> <p>Suppose we have a collection of documents, each represented as a numerical vector in a high-dimensional space. These vectors capture the semantic meaning and features of the documents.</p> <p>For simplicity, let‚Äôs consider a small collection of three documents represented by their vectors:</p> <p>Documents: $A, B, C$ Here, document(s) refer to the chunck of texts from any sources. \(A: [0.2, 0.5, 0.8] \space B: [0.7, 0.3, 0.6] \space C: [0.4, 0.9, 0.1]\)</p> <p>Now, let‚Äôs say we have a query document represented by the vector $[0.6, 0.4, 0.7]$. We want to find the most similar document in our collection to this query document.</p> <p>In a vector database, document similarity is computed using techniques such as cosine similarity or Euclidean distance. Let‚Äôs use cosine similarity for this example:</p> <p>Cosine similarity between two vectors A and B is calculated as the cosine of the angle between them:</p> \[\text{cosine similarity} = \frac{A \cdot B}{\|A\| \|B\|}\] <p>Where:</p> <ul> <li>$ A \cdot B $ is the dot product of vectors A and B.</li> <li>$ |A| $ and $ |B| $ are the magnitudes of vectors A and B, respectively.</li> </ul> <p>Using cosine similarity, we compute the similarity between the query vector [0.6, 0.4, 0.7] and each document vector in the collection:</p> <ul> <li>Cosine similarity between query and document A: $ \frac{(0.6 \times 0.2) + (0.4 \times 0.5) + (0.7 \times 0.8)}{\sqrt{0.6^2 + 0.4^2 + 0.7^2} \times \sqrt{0.2^2 + 0.5^2 + 0.8^2}} \approx 0.93 $</li> <li>Cosine similarity between query and document B: $ \frac{(0.6 \times 0.7) + (0.4 \times 0.3) + (0.7 \times 0.6)}{\sqrt{0.6^2 + 0.4^2 + 0.7^2} \times \sqrt{0.7^2 + 0.3^2 + 0.6^2}} \approx 0.91 $</li> <li>Cosine similarity between query and document C: $ \frac{(0.6 \times 0.4) + (0.4 \times 0.9) + (0.7 \times 0.1)}{\sqrt{0.6^2 + 0.4^2 + 0.7^2} \times \sqrt{0.4^2 + 0.9^2 + 0.1^2}} \approx 0.81 $</li> </ul> <p>Based on the computed cosine similarities, document A is the most similar to the query document. Therefore, in response to the query, the vector database would return document A as the most similar document to the query document $[0.6, 0.4, 0.7]$.</p> <div class="fake-img l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/v_db-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/v_db-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/v_db-1400.webp"/> <img src="/assets/img/v_db.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong><em>Amazing!! What if we have extremely large number of documents?</em></strong></p> <p>There are many techniques to address this issue. However, simple one would be to use metadata in each document. It is effective if we are aware of metadata for document clustering that narrows down the computation. In addition, we can employ algorithm such as Hierarchical Navigable Small Worlds (HNSW).</p> <p>Following code returns a vector retriever with only matching filter i.e. metadata.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pinecone example
</span><span class="n">search_kwargs</span> <span class="o">=</span> <span class="p">{</span> <span class="sh">"</span><span class="s">filter</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span> <span class="sh">"</span><span class="s">meta_key</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">metadata_value</span><span class="sh">"</span> <span class="p">}}</span>
<span class="k">return</span> <span class="n">vectore_store</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span>
    <span class="n">search_kwargs</span><span class="o">=</span><span class="n">search_kwargs</span>
<span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="code">Code</h2> <p>A quick reminder, why are we using RAG ? Once or often LLM training and fine-tuning is highly expensive. In addition, some data are highly confidential but need tools like LLM. To address aforementioned issues, we retrieve and augment the LLM capabilities.</p> <p>Lets recall the RAG pipeline section, and use some software engineering skills to develop LLM powered RAG applications. Here, we will just outline the basics of RAG codes using tools such as langchain, SentenceTransformerEmbeddings, OpenAI ChatLLM, and pinecone as a vector database.</p> <p>Following code will demo a pdf based RAG app. Yet, lanchain and tools have magical abilities.</p> <pre><code class="language-env">OPENAI_API_KEY=sk-

PINECONE_API_KEY=
PINECONE_ENV_NAME=
PINECONE_INDEX_NAME=example

</code></pre> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Extra</span>
<span class="k">class</span> <span class="nc">Metadata</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">extra</span><span class="o">=</span><span class="n">Extra</span><span class="p">.</span><span class="n">allow</span><span class="p">):</span>
    <span class="n">conversation_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">user_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">doc_id</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">class</span> <span class="nc">ChatArgs</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">extra</span><span class="o">=</span><span class="n">Extra</span><span class="p">.</span><span class="n">allow</span><span class="p">):</span>
    <span class="n">conversation_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">doc_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">Metadata</span>
    <span class="n">streaming</span><span class="p">:</span> <span class="nb">bool</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>

<span class="k">def</span>  <span class="nf">build_llm</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span> <span class="n">ChatArgs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatOpenAI</span><span class="p">:</span>
    <span class="k">return</span> <span class="nc">ChatOpenAI</span><span class="p">()</span>
</code></pre></div></div> <p>The reason to use SentenceTransformerEmbeddings while using OpenaAI as LLM is OpenaAI has API limiter. If you have pro access to LLMs like OpenAI GPT-*, just replace the follwoing code block with OpenAI embeddings.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">SentenceTransformerEmbeddings</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">SentenceTransformerEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">pinecone</span>
<span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Pinecone</span> 
<span class="kn">from</span> <span class="n">app.chat.embeddings.sentence_transformer</span> <span class="kn">import</span> <span class="n">embeddings</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>

<span class="n">pinecone</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">PINECONE_API_KEY</span><span class="sh">"</span><span class="p">),</span> <span class="n">environment</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">PINECONE_ENV_NAME</span><span class="sh">"</span><span class="p">))</span>

<span class="n">vectore_store</span> <span class="o">=</span> <span class="n">Pinecone</span><span class="p">.</span><span class="nf">from_existing_index</span><span class="p">(</span><span class="n">index_name</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">PINECONE_INDEX_NAME</span><span class="sh">"</span><span class="p">),</span> <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>


<span class="sh">'''</span><span class="s">
We are filtering docs based on metadata such as doc_id. Hence, the document retriever has less document to query against.
</span><span class="sh">'''</span>
<span class="k">def</span> <span class="nf">build_retriever</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span><span class="n">ChatArgs</span><span class="p">):</span>
    <span class="n">search_kwargs</span> <span class="o">=</span> <span class="p">{</span> <span class="sh">"</span><span class="s">filter</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span> <span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">chat_args</span><span class="p">.</span><span class="n">pdf_id</span> <span class="p">}}</span>
    <span class="k">return</span> <span class="n">vectore_store</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span>
        <span class="n">search_kwargs</span><span class="o">=</span><span class="n">search_kwargs</span>
    <span class="p">)</span>

</code></pre></div></div> <p>The following code is just to add history to each chat conversation. [Optional]</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="n">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationBufferMemory</span>
<span class="kn">from</span> <span class="n">langchain.schema</span> <span class="kn">import</span> <span class="n">BaseChatMessageHistory</span>

<span class="kn">from</span> <span class="n">app.web.api</span> <span class="kn">import</span> <span class="n">get_messages_by_conversation_id</span><span class="p">,</span> <span class="n">add_message_to_conversation</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>

<span class="c1">##overriding the default BaseChatMessageHistory
</span><span class="k">class</span> <span class="nc">DemoMessageHistory</span><span class="p">(</span><span class="n">BaseChatMessageHistory</span><span class="p">,</span> <span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">conversation_id</span><span class="p">:</span> <span class="nb">str</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">messages</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">get_messages_by_conversation_id</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">conversation_id</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">add_message</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">add_message_to_conversation</span><span class="p">(</span><span class="n">conversation_id</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">conversation_id</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">message</span><span class="p">.</span><span class="nb">type</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">clear</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    
<span class="k">def</span> <span class="nf">build_memory</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span> <span class="n">ChatArgs</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">ConversationBufferMemory</span><span class="p">(</span>
        <span class="n">chat_memory</span><span class="o">=</span><span class="nc">DemoMessageHistory</span><span class="p">(</span><span class="n">conversation_id</span><span class="o">=</span><span class="n">chat_args</span><span class="p">.</span><span class="n">conversation_id</span><span class="p">),</span>
        <span class="n">return_messages</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">memory_key</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">output_key</span><span class="o">=</span><span class="sh">"</span><span class="s">answer</span><span class="sh">"</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Generate and store embeddings for the given documnent</p> <ul> <li>Extract text from the specified document.</li> <li>Divide the extracted text into manageable chunks.</li> <li>Generate an embedding for each chunk.</li> <li>Persist the generated embeddings to pinecone.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">PyPDFLoader</span>
<span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="n">app.chat.vector_stores.pinecone</span> <span class="kn">import</span> <span class="n">vectore_store</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="k">def</span> <span class="nf">create_embeddings_for_docs</span><span class="p">(</span><span class="n">doc_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">doc_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    @params
    doc_id: The unique identifier for the doc. 
    doc_path: The file path to the doc.
    Usage:
    create_embeddings_for_docs(</span><span class="sh">'</span><span class="s">123456</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">/path/to/pdf</span><span class="sh">'</span><span class="s">)
    </span><span class="sh">"""</span>

    <span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1600</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span><span class="nc">PyPDFLoader</span><span class="p">(</span><span class="n">pdf_path</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load_and_split</span><span class="p">(</span><span class="n">text_splitter</span><span class="o">=</span><span class="n">text_splitter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
        <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span> <span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">:</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">],</span>
            <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">pdf_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">pdf_id</span>
        <span class="p">}</span>
    <span class="n">vectore_store</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

</code></pre></div></div> <p>This piece of code will run when we add new embeddings from new documents to vector database.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">app.web.db.models</span> <span class="kn">import</span> <span class="n">Docs</span>
<span class="kn">from</span> <span class="n">app.web.files</span> <span class="kn">import</span> <span class="n">download</span>
<span class="kn">from</span> <span class="n">app.chat</span> <span class="kn">import</span> <span class="n">create_embeddings_for_pdf</span>

<span class="k">def</span> <span class="nf">process_document</span><span class="p">(</span><span class="n">doc_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">Docs</span><span class="p">.</span><span class="nf">find_by</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">doc_id</span><span class="p">)</span>
    <span class="k">with</span> <span class="nf">download</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span> <span class="k">as</span> <span class="n">doc_path</span><span class="p">:</span>
        <span class="nf">create_embeddings_for_docs</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>

</code></pre></div></div> <p>Now, vector database is ready to query. Let build chat.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">ConversationalRetrievalChain</span>
<span class="kn">from</span> <span class="n">app.chat.models</span> <span class="kn">import</span> <span class="n">ChatArgs</span>
<span class="kn">from</span> <span class="n">app.chat.vector_stores.pinecone</span> <span class="kn">import</span> <span class="n">build_retriever</span>
<span class="kn">from</span> <span class="n">app.chat.llms.chatopenai</span> <span class="kn">import</span> <span class="n">build_llm</span>
<span class="kn">from</span> <span class="n">app.chat.memories.sql_memory</span> <span class="kn">import</span> <span class="n">build_memory</span>

<span class="k">def</span> <span class="nf">build_chat</span><span class="p">(</span><span class="n">chat_args</span><span class="p">:</span> <span class="n">ChatArgs</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    @params
    chat_args: ChatArgs object containing
    conversation_id, doc_id, metadata, and streaming flag.
    @return: chain
    Usage:
    chain = build_chat(chat_args)
    </span><span class="sh">"""</span>
    <span class="n">retriever</span> <span class="o">=</span> <span class="nf">build_retriever</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="nf">build_llm</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="nf">build_memory</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ConversationalRetrievalChain</span><span class="p">.</span><span class="nf">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span> <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">)</span>

</code></pre></div></div> <p>running the chat chain:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">input</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Which city has the most temples in Nepal?</span><span class="sh">"</span>
<span class="n">chat_args</span> <span class="o">=</span> <span class="nc">ChatArgs</span><span class="p">(</span>
        <span class="n">conversation_id</span><span class="o">=</span><span class="n">conversation</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
        <span class="n">doc_id</span><span class="o">=</span><span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
        <span class="n">streaming</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">conversation_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">conversation</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">user_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">g</span><span class="p">.</span><span class="n">user</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="n">chat</span> <span class="o">=</span> <span class="nf">build_chat</span><span class="p">(</span><span class="n">chat_args</span><span class="p">)</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>We have walked through simple RAG application process, why it is required and what it can do? In addition, we explored technologies like large language models, sentence embeddings, sentence BERT, vector database and langchian. We also expolred simple example of RAG pipeline. Yet, tools like langchain, and llamaindex can unlock complex systems with agents.</p> <p>In order to have deep understanding of future apps employing Generative AI, one must understand the inner working atleast of transformer model.</p> <hr/> <h2 id="citations">Citations</h2> <p>Thanks to GenAI for content writing.</p>]]></content><author><name>Tsuyog Basnet</name></author><category term="RAG"/><category term="AIapplicaitons"/><summary type="html"><![CDATA[A Guide to understand and build RAG application, with Embedding, Sentence BERT, Vector Database, and LLMs]]></summary></entry><entry><title type="html">DistilBERT - Knowledge distillation from pretrained BERT</title><link href="https://pvsnp9.github.io/blog/2024/knowledge_dist/" rel="alternate" type="text/html" title="DistilBERT - Knowledge distillation from pretrained BERT"/><published>2024-04-03T00:00:00+00:00</published><updated>2024-04-03T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/knowledge_dist</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/knowledge_dist/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/knowledge_distillation.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="Transformer"/><category term="BERT"/><category term="DistilBERT"/><summary type="html"><![CDATA[How to distill knowledge from pretrained BERT model. A teacher student relationship.]]></summary></entry><entry><title type="html">TinyBERT - Knowledge distillation from pretrained BERT</title><link href="https://pvsnp9.github.io/blog/2024/tinby_bert/" rel="alternate" type="text/html" title="TinyBERT - Knowledge distillation from pretrained BERT"/><published>2024-04-03T00:00:00+00:00</published><updated>2024-04-03T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/tinby_bert</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/tinby_bert/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/tinybert.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="Transformer"/><category term="BERT"/><category term="TinyBERT"/><summary type="html"><![CDATA[How to distill knowledge from pretrained BERT model. A teacher student relationship.]]></summary></entry><entry><title type="html">BERT Basics and Fine Tunning</title><link href="https://pvsnp9.github.io/blog/2024/bert_basics/" rel="alternate" type="text/html" title="BERT Basics and Fine Tunning"/><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/bert_basics</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/bert_basics/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/bert_basics.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="Transformer"/><category term="Transformer"/><category term="BERT"/><category term="NLP"/><summary type="html"><![CDATA[Transformer for Natural Language Processing. Learn How to use pretrained BERT models, fine-tune them and decipher their working.]]></summary></entry><entry><title type="html">Named Entity Recognition With BERT</title><link href="https://pvsnp9.github.io/blog/2024/ner/" rel="alternate" type="text/html" title="Named Entity Recognition With BERT"/><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/ner</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/ner/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/ner.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="Transformer"/><category term="NER"/><category term="BERT"/><category term="NLP"/><summary type="html"><![CDATA[Learn How to Fine-Tune BERT models for NER and unserstand their working.]]></summary></entry><entry><title type="html">QnA with BERT</title><link href="https://pvsnp9.github.io/blog/2024/qa_fine_tune/" rel="alternate" type="text/html" title="QnA with BERT"/><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/qa_fine_tune</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/qa_fine_tune/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/qa_fine_tune.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="Transformer"/><category term="QnA"/><category term="BERT"/><category term="NLP"/><summary type="html"><![CDATA[Transformer for Natural Language Processing. Learn How to Fine-Tune BERT models for QnA and unserstand their working.]]></summary></entry><entry><title type="html">BERT Variants and Their illustrations</title><link href="https://pvsnp9.github.io/blog/2024/variants/" rel="alternate" type="text/html" title="BERT Variants and Their illustrations"/><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/variants</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/variants/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/variants.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="Transformer"/><category term="ALBERT"/><category term="RoBERTa"/><category term="ELECTRA"/><category term="SpanBERT"/><category term="NLP"/><summary type="html"><![CDATA[Learn different variants of BERT models such as ALBERT, RoBERTa, ELECTRA, and SpanBET and their inner workings with exampel.]]></summary></entry><entry><title type="html">Data and Sampling</title><link href="https://pvsnp9.github.io/blog/2024/data-sampling/" rel="alternate" type="text/html" title="Data and Sampling"/><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/data-sampling</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/data-sampling/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/data_and_sampling.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="data-science"/><category term="stattistics"/><category term="data-science"/><summary type="html"><![CDATA[Data and Sampling from a book Practical Statistics for Data Scientists]]></summary></entry><entry><title type="html">Exploratory Data Analysis</title><link href="https://pvsnp9.github.io/blog/2024/jupyter-notebook/" rel="alternate" type="text/html" title="Exploratory Data Analysis"/><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T00:00:00+00:00</updated><id>https://pvsnp9.github.io/blog/2024/jupyter-notebook</id><content type="html" xml:base="https://pvsnp9.github.io/blog/2024/jupyter-notebook/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/eda.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="data-science"/><category term="stattistics"/><category term="data-science"/><summary type="html"><![CDATA[Exploring Basics of EDA from a book Practical Statistics for Data Scientists]]></summary></entry></feed>