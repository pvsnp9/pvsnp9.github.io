<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Audio classification using Deep learning | Tsuyog Basnet</title> <meta name="author" content="Tsuyog Basnet"> <meta name="description" content="A project on audio classification using convolutional neural network, and recurrent neural network. This work was carried out in 2019, but still explains fundamentals on audio processing for deeplearning."> <meta name="keywords" content="Machine-Learninng, Data-Scientist, researcher, NLP, personal-blog"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pvsnp9.github.io/projects/audio_classification/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Tsuyog Basnet</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Experiments</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Audio classification using Deep learning</h1> <p class="post-description">A project on audio classification using convolutional neural network, and recurrent neural network. This work was carried out in 2019, but still explains fundamentals on audio processing for deeplearning.</p> </header> <article> <p>We’re surrounded by lots of sound, and our brains are always figuring out what they mean. Music is one type of sound that we really like. I worked on a project where I taught a computer to tell different musical instruments apart just by listening. Using deep learning, I made a system that could do this with 95% accuracy using something called a convolutional neural network (CNN), and 85% accuracy with another type of network called a recurrent neural network (RNN). In this report, I’ll explain how I did it - from gathering the data, to teaching the computer, to checking how well it learned.</p> <h2>Introduction</h2> <p>In our everyday lives, we're surrounded by sound. Whether it's music or background noise, our ears are always picking up on what's going on around us. Every sound comes from somewhere and travels to our ears, where our brains figure out what it means.</p> <p> Music is something we all enjoy, and technology is getting better at recognizing different musical instruments just by listening to them. We're using fancy computer techniques, like deep learning, to teach computers how to do this. These methods have already been successful in things like recognizing images and understanding language, and now we're applying them to sounds too.</p> <p>This project is all about using these techniques to figure out which musical instruments are making the sounds we hear. We're focusing on things like guitars, drums, and pianos, and we're trying to make the computer really good at telling them apart.</p> <p>This kind of technology isn't just useful for music. It can help with all sorts of things, like sorting through audio recordings, figuring out what language someone is speaking, or even helping people who have trouble hearing.</p> <h2>Problem Statement</h2> <p>The main goal of this project is to use both audio processing and deep learning to sort music based on the instruments playing it. Here's how it'll work: You feed the program a short audio file, like a .wav file, and it'll give you a score for the most likely instrument being played, like saxophone or guitar. If the program hears a new sound it hasn't been trained on, it'll still try to make a guess about what instrument it might be. To make all this happen, I'll be using various audio processing tricks to get the data ready, and then I'll employ two types of deep learning: convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for the actual sorting process. I'll explain each step in detail as we go along.</p> <h2>Audio Signals</h2> <h3>Sampling &amp; Sampling Frequency</h3> <p> Sampling frequency refers to how often we take these snapshots, or samples, per second. It's measured in Hertz (Hz), which tells us how many samples we take in one second. For example, if we have a sampling frequency of 44.1 kHz (kilohertz), it means we're taking 44,100 samples every second. <br> <i><b>Why does this matter?</b></i> Well, it's crucial for accurately representing the original audio signal. The higher the sampling frequency, the more faithfully we can capture the details of the signal. This is particularly important for reproducing high-frequency sounds, like those found in music. However, there's a limit to how high we can set the sampling frequency. This is determined by the Nyquist-Shannon sampling theorem, which states that in order to accurately reconstruct a signal, the sampling frequency must be at least twice the highest frequency present in the signal itself. This means that if a signal contains frequencies up to 20 kHz (the upper limit of human hearing), then the sampling frequency should be at least 40 kHz to capture all the details. In summary, sampling in audio signal processing involves taking snapshots of an audio signal at regular intervals, while the sampling frequency determines how often these snapshots are taken per second. Choosing an appropriate sampling frequency is crucial for accurately representing the original audio signal without losing any important information. </p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sr-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sr-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sr-1400.webp"></source> <img src="/assets/img/sr.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Sample rate" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Sample Rate</div> <h3>Amplitude</h3> <p> Amplitude is the magnitude or intensity of a sound wave. In simpler terms, it tells us how loud or soft a sound is. When you visualize an audio waveform, the amplitude is represented by the height of the waveform at any given point. In technical terms, amplitude is measured in decibels (dB) and represents the variation in air pressure caused by the sound wave. A higher amplitude corresponds to a greater variation in air pressure, resulting in a louder sound, while a lower amplitude indicates a softer sound. Amplitude is a fundamental characteristic of sound and plays a crucial role in how we perceive and interpret audio. It affects our perception of volume, dynamics, and overall sound quality. Understanding and controlling amplitude is essential in fields such as audio engineering, where precise control over sound levels is necessary to achieve desired outcomes in recording, mixing, and mastering audio tracks. Summary, amplitude is the measure of the intensity or loudness of a sound wave, expressed in decibels, and is a fundamental aspect of sound perception and audio signal processing. </p> <h3>Fourier Transform</h3> <p> The Fourier Transform is a mathematical tool used in signal processing to analyze and decompose complex signals into simpler components. Named after the French mathematician Joseph Fourier, who developed the concept in the early 19th century, it's a powerful technique that's widely used in various fields, including audio processing, image analysis, and telecommunications. At its core, the Fourier Transform works by breaking down a signal into its individual frequency components. You can think of it like breaking down a complex musical chord into its individual notes. By doing this, we can understand the different frequencies that make up the original signal and how much of each frequency is present. In practical terms, the Fourier Transform takes a time-domain signal, which represents the signal's amplitude over time, and converts it into a frequency-domain signal, which represents the signal's amplitude at different frequencies. This transformation allows us to analyze the frequency content of the signal and identify important features such as dominant frequencies, harmonics, and noise. There are different variants of the Fourier Transform, including the Discrete Fourier Transform (DFT) and the Fast Fourier Transform (FFT). The FFT is particularly popular because it allows for fast computation of the Fourier Transform, making it practical for real-time applications and digital signal processing. Fourier Transform is a mathematical technique used to analyze signals by breaking them down into their constituent frequency components. It's a fundamental tool in signal processing that's used to extract useful information from complex signals and has applications in various fields, including audio processing, telecommunications, and image analysis. </p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ft-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ft-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ft-1400.webp"></source> <img src="/assets/img/ft.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Fourier Transform" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Top digital Signal, bottom Fourier transform of the signal</div> <p> we will be using short time Fourier transform that is implemented in librosa. Fourier is an elegant way to decompose an audio signal into its constituent frequency. </p> <h3>Periodogram</h3> <p> The periodogram is a method used in signal processing to estimate the power spectrum of a signal. In simpler terms, it helps us understand the frequency content and power distribution of a signal. Imagine you have a signal, like a piece of music or a sound recording. The periodogram breaks down this signal into different frequency bands and tells us how much power or energy is present in each band. The process starts by dividing the signal into smaller segments, typically overlapping sections, to ensure that we capture all the frequency components accurately. Then, for each segment, we apply a Fourier Transform to convert the signal from the time domain to the frequency domain. This gives us a representation of the signal's frequency content. Next, we calculate the squared magnitude of the Fourier Transform for each segment. This squared magnitude represents the power or energy of the signal at each frequency. Finally, we average these squared magnitudes across all segments to obtain the periodogram, which shows us the distribution of power across different frequencies in the signal. The periodogram is useful for analyzing signals with varying frequency content, such as music or speech. It allows us to identify dominant frequencies, detect periodic patterns, and distinguish between signal and noise components. The periodogram is a method used in signal processing to estimate the power spectrum of a signal by analyzing its frequency content. It provides valuable insights into the characteristics of a signal and is widely used in fields such as audio processing, telecommunications, and vibration analysis. </p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/per-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/per-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/per-1400.webp"></source> <img src="/assets/img/per.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Periodogram" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Periodogram [Source: wiki]</div> <h3>Spectral Density </h3> <p> Spectral density describes how the power of a signal is distributed across different frequencies. In simpler terms, it tells us how much power or energy is present in the signal at each frequency. Imagine you have a signal, like a piece of music or a sound recording. Spectral density helps us understand how much power or energy is concentrated in various frequency bands within that signal. Mathematically, spectral density is often represented as a function <i>S(f) </i>, where <i> f</i> represents frequency. This function describes the power or energy of the signal at each frequency <i> f </i>. In continuous-time signals, spectral density is typically represented in terms of power per unit frequency (e.g., watts per hertz), while in discrete-time signals, it's often represented in terms of power per unit frequency bin. The spectral density function provides valuable information about the frequency content of a signal. For example, it can help us identify dominant frequencies, detect periodic patterns, and distinguish between signal and noise components. By analyzing the spectral density of a signal, we can gain insights into its characteristics and better understand its underlying properties. Spectral density estimation is a common task in signal processing, and various techniques, such as the periodogram, Welch's method, and multitaper methods, are used to estimate spectral density from sampled data. These methods allow us to analyze the frequency content of signals in both time and frequency domains, enabling us to extract useful information and make informed decisions in various applications, including audio processing, telecommunications, and vibration analysis. </p> <h3>Mel-Scale </h3> <p> The Mel scale is a perceptual scale that maps frequencies to pitches in a way that approximates the human auditory system's response to different frequencies. Named after the scientist Steven Mels, it's widely used in audio signal processing, particularly in fields like speech recognition and music analysis. Here's how it works: The Mel scale is based on the idea that our perception of pitch is not linearly related to the actual frequency of a sound. Instead, we're more sensitive to changes in pitch at lower frequencies compared to higher frequencies. For example, we can easily distinguish between two tones that are an octave apart at low frequencies, but the same difference is less noticeable at higher frequencies. To account for this non-linear perception, the Mel scale uses a transformation that compresses lower frequencies and expands higher frequencies. This transformation is achieved through a series of mathematical operations, resulting in a scale where equal intervals correspond to equal perceptual differences in pitch. The formula for converting from frequency <i>f</i> to Mel scale <i>m</i> is typically given by: $$ m = 2595 \log_{10}(1 + \frac{f}{700}) $$ Conversely, to convert from Mel scale back to frequency, we use the inverse formula: $$ f = 700 (10^{\frac{m}{2595}} - 1) $$ The Mel scale is particularly useful in audio processing tasks where human perception of sound is important, such as speech recognition and music analysis. By using the Mel scale, we can design algorithms and systems that better mimic the way humans perceive and process auditory information, leading to improved performance and accuracy in these applications. </p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mel-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mel-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mel-1400.webp"></source> <img src="/assets/img/mel.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Mel-scale" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Pitch-scale and Mel-Scale [Source: wiki]</div> <h3>Cepstrum</h3> <p> The cepstrum is a powerful technique used in signal processing to analyze the spectral characteristics of a signal by examining its frequency components in a unique way. The term "cepstrum" is derived from "spectrum" spelled backward, highlighting its relationship with spectral analysis. Here's how it works: <b>Calculate the Fourier Transform</b>: The first step in computing the cepstrum involves taking the Fourier Transform of the signal. This converts the signal from the time domain to the frequency domain, revealing its frequency content. <b>Take the Logarithm</b>: Next, we take the logarithm of the magnitude spectrum obtained from the Fourier Transform. This helps to emphasize the relative amplitudes of different frequency components. <b>Inverse Fourier Transform</b>: After taking the logarithm, we perform an inverse Fourier Transform on the resulting spectrum. This converts the spectrum back from the frequency domain to the time domain. <b>Cepstrum</b>: The resulting signal is known as the cepstrum. It represents the "quefrency" domain, which is the domain of the inverse Fourier Transform of the logarithm of the spectrum. In other words, it captures information about the rate of change of the spectral components of the original signal. </p> <h3>Spectrogram</h3> <p> A spectrogram is a visual representation of the frequency content of a signal as it changes over time. It's like taking a snapshot of the frequency components of a signal at different moments and stitching them together to create a dynamic picture. Here's how it's done: <b>Dividing the Signal</b>: First, we divide the signal into small segments, typically using a technique called windowing. Each segment represents a short duration of the signal, such as a fraction of a second. <b>Applying the Fourier Transform</b>: Next, we apply the Fourier Transform to each segment. This converts the signal from the time domain to the frequency domain, revealing its frequency content. <b>Power Spectrum Calculation</b>: We then calculate the power spectrum of each segment. This tells us how much power or energy is present at each frequency within that segment. <b>Building the Spectrogram</b>: Finally, we arrange the power spectra of all segments over time to create the spectrogram. Typically, the horizontal axis represents time, the vertical axis represents frequency, and the intensity or color represents the power or energy level at each frequency and time point. In summary, spectrograms are a valuable tool for understanding the frequency content and temporal evolution of signals, enabling us to uncover patterns, identify features, and extract meaningful information from audio and other types of signals. </p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/spec-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/spec-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/spec-1400.webp"></source> <img src="/assets/img/spec.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Spectrogram" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Mel-frequency spectrogram [Source: wiki]</div> <h4><a href="https://medium.com/codex/understanding-convolutional-neural-networks-a-beginners-journey-into-the-architecture-aab30dface10" target="_blank" rel="external nofollow noopener"> Convolutional Neural Network Blog</a></h4> <h4> <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external nofollow noopener">RNN &amp; LSTM Blog</a> </h4> <p>The deep learning concepts like CNN and RNN can be a different article. Please use the link to understand those concepts.</p> <h3>Methodologies</h3> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/aud_cls_process-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/aud_cls_process-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/aud_cls_process-1400.webp"></source> <img src="/assets/img/aud_cls_process.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Porcess" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Process outline</div> <p>First we process the audio signal by extracting Mel-frequency cepstral coefficients (MFCCs) from audio smaples per-frame basis. The MFCC summarizes the frequency distribution over window size that enables us to analyze the frequency and features for classification. Once the preprocessing is complete we use CNN and RNN for classificatoin. However, this report particularly focuses on audio signal processing part.</p> <p>That <a href="https://www.kaggle.com/c/freesound-audio-tagging/data" target="_blank" rel="external nofollow noopener"> Dataset</a> is taken from Kaggle. It consists of 300 audio samples and 10 different class labels with instrument.csv file.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/aud_cls_dist-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/aud_cls_dist-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/aud_cls_dist-1400.webp"></source> <img src="/assets/img/aud_cls_dist.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Instruments class" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/aud_cls_pie-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/aud_cls_pie-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/aud_cls_pie-1400.webp"></source> <img src="/assets/img/aud_cls_pie.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Pie-chart for instrument class" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Instrument classes and their distribution. </div> <p>Lets further discuss the tools setup and pre-processing audio data. The dataset is 16-bit PCM and sampling rate of 44.1 kHz (44100 samples per second). The primary purpose is to compute MFCC and spectrogram of the audio signals. The audio signals are already preprocessed for window function which includes pre-emphasis, and framing.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/aud_prep-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/aud_prep-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/aud_prep-1400.webp"></source> <img src="/assets/img/aud_prep.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Sample data" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Time series audio signals from librosa</div> <h4>Fourier Transform</h4> <p>Lets convert the time domain audio signal to frequency domain.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_fft</span><span class="p">(</span><span class="n">signal</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
    <span class="sh">""""</span><span class="s">
    @params: signal, and sampling rate
    @return: mean normalized magnitude, and transformed frequency
    </span><span class="sh">""""</span><span class="s">

    signal_length = len(signal)
    frequency = np.fft.rfftfreq(signal_length, d = 1/rate)
    #mean normalization of length of signal
    magnitude = abs(np.fft.rfft(signal)/signal_length) 
    return (magnitude, frequency)
</span></code></pre></div></div> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fft-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fft-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fft-1400.webp"></source> <img src="/assets/img/fft.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="FFT" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Magnitude and FFT signal</div> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fft_sample-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fft_sample-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fft_sample-1400.webp"></source> <img src="/assets/img/fft_sample.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="FFT Sample" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">FFT signal sample for each instruments</div> <h4>Mel-Scale</h4> <p>We first compute filter bank using triangular filter. By default, the number of filters in Mel-scale are 40. For this instnace, lets use 26 standard filters to compute filter bank.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">python_speech_features</span> <span class="kn">import</span> <span class="n">mfcc</span><span class="p">,</span> <span class="n">logfbank</span>
<span class="err">﻿</span><span class="nf">logfbank</span><span class="p">(</span><span class="n">signal</span><span class="p">[:</span><span class="n">rate</span><span class="p">],</span> <span class="n">rate</span><span class="p">,</span> <span class="n">nfilt</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">nfft</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</code></pre></div></div> <p>The above function produces the array of size filter (26) by nfft i.e. 512 (default). Lets examine the coefficients for all classes.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/f_bank_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/f_bank_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/f_bank_1-1400.webp"></source> <img src="/assets/img/f_bank_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Filter Bank spectrogram" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Filter Bank spectrogram of all instruments</div> <p>The spectogram seems correlated among classes that leads deep learning models to confusion. Lets correct them by applying discrete cosine transformation as it decorrelates the filter bank coefficient and yields a compressed representation of filter banks. The spectral dimension is 26 by 512, lets reduce the size to 13 by 99 cepstral coefficients because fast changing filter bank coefficients do not carry additional information. New spectogram of 13 by 99 looks following.</p> <div class="row justify-content-sm-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/f_bank_cepstrum-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/f_bank_cepstrum-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/f_bank_cepstrum-1400.webp"></source> <img src="/assets/img/f_bank_cepstrum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="cepstralspectrogram" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Mel frequency cepstrum coefficient spectrogram</div> <p>Now, we have input ready for CNN and RNN to classify instruments. The input size of each instance is 13 by 99. From this point, i will let you to use Tensorflow or pytorch to build deep learning models. <a href="https://github.com/pvsnp9/audio_classification_using_deep_learning/tree/master" target="_blank" rel="external nofollow noopener">Code</a></p> <h2>Conclusion</h2> <p>We explored the audio signal processing for deep learning models. Since, the development of LLM models, CNN and RNNs are frivolus. The GenAI has ability to create and compose music. The main objective of this article is to provide the understanding the basic pieces od audio signals, and will definately bolsters use them in GenAI.</p> <h2>References</h2> <p><b><a href="https://www.kaggle.com/c/freesound-audio-tagging/data" target="_blank" rel="external nofollow noopener">Kaggle</a></b></p> <p><b><a href="https://www.haythamfayek.com/%202016/04/21/speech-processing-for-machine-learning.html" target="_blank" rel="external nofollow noopener">Speech processing for machine learning</a></b></p> <p><b><a href="https://www.%20Practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs" target="_blank" rel="external nofollow noopener">Mel Frequency Cepstral Coefficient (MFCC) tutorial</a></b></p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Tsuyog Basnet. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 16, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>